{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1sWJ6YdDIA_qYz2gP6jkBb5-P9nZ9C58H","timestamp":1741074329053},{"file_id":"17b2xTtIoErmyvv9Nq-wXHWd5YUWaOLei","timestamp":1741042986285},{"file_id":"13S1XqujQb_510jIZtlGuNJQIuHFKY3Cc","timestamp":1741031812351},{"file_id":"1w4p6q7r7nkLoiREGEWS1Q3j2FH_rhk8j","timestamp":1711408526825},{"file_id":"17hDYTaaNNmwgGEhOlzHqzH9SqREpxx91","timestamp":1711352680764}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Probe-Klausur abgegeben von: Briggen, Roger CAS AI HS 24 **100 Punkte**\n","\n","---\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"RyFprCKICO34"}},{"cell_type":"code","source":[],"metadata":{"id":"Pqex1dbPHwxp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1 Dense Network **50 Punkte**\n","\n","\n","---\n","\n","\n","\n"],"metadata":{"id":"QL3vXpgJCWBg"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"9rhep4xNXVYt","executionInfo":{"status":"ok","timestamp":1741077602288,"user_tz":-60,"elapsed":12279,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}}},"outputs":[],"source":["import torch\n","import numpy as np\n","from torch import nn\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from torch.utils.data import TensorDataset, DataLoader, Dataset\n","from sklearn.metrics import accuracy_score\n"]},{"cell_type":"markdown","source":["Wir definieren folgende **trainierbare Netzwerkparameter**:"],"metadata":{"id":"Nd7kEl34J8Nr"}},{"cell_type":"code","source":["# Die vorhandenen Werte bitte nicht verändern\n","# 1. Layer\n","# Gewichte\n","Target_W_1 = torch.randn(32, 16)  # 16 Eingänge, 32 Ausgänge\n","# Bias\n","Target_b_1 = torch.randn(32) # hier code   32 Neuronen\n","\n","# 2. Layer\n","# Gewichte\n","Target_W_2 = torch.randn(32, 5) # hier code  muss mit W1 Output übereinstimmen\n","# Bias\n","Target_b_2 = torch.randn(5) # hier code\n"],"metadata":{"id":"-8wBUF23yd7j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Datensatz\n","X = torch.randn((100, 16)) # hier code  100 ist beliebig, 16 muss übereinstimmen mit dem Input in L1\n"],"metadata":{"id":"wpUWtN6IXikB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1.1 Vervollständigen Sie bitte den Code und bestimmen Sie: **18 Punkte**\n","\n","1. Vervollständigen Sie bitte den Code an den **fünf (5)** Stellen: missing 1, 2, 3, 4, 5 - **10 Punkte**\n","> Beachten Sie dabei, dass:\n","> - Die Layer 1 und 2 werden aufeinander gestappelt\n","> - Die Datensatz-Länge ist frei wählbar (jedoch nicht die Anzahl der Spalten/Variablen)\n","2. Nachdem Sie fehlende Grössen eingefügt haben, können Sie jetzt die Anzahl der trainable Parameter pro Layer bestimmen - **8 Punkte**\n","\n","**Layer 1:**\n","\n","Gewichte: 32 × 16 = 512\n","\n","Bias: 32\n","\n","---------------------\n","\n","Gesamt: 544 Parameter\n","\n","\n","**Layer 2:**\n","\n","Gewichte: 32 × 5 = 160\n","\n","Bias: 5\n","\n","---------------------\n","Gesamt: 165 Parameter\n","\n","**Total: 544 + 165 = 709 Parameter**\n","\n"],"metadata":{"id":"Y04-Wmx1Rjci"}},{"cell_type":"markdown","source":["Gegeben sind 4 Images **unterscheidlicher Grösse**\n","\n"],"metadata":{"id":"Ap7ecUb_SpX-"}},{"cell_type":"code","source":["Image1 = torch.rand((3, 255, 255))\n","Image2 = torch.rand((3, 320, 320))\n","Image3 = torch.rand((3, 320, 320))\n","Image4 = torch.rand((3, 120, 120))"],"metadata":{"id":"fgazLouj3HEi","executionInfo":{"status":"ok","timestamp":1741080215005,"user_tz":-60,"elapsed":42,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["class NeuralNetwork(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.flatten = nn.Flatten()\n","        self.linear_relu_stack = nn.Sequential(\n","            ## hier Layer code\n","            nn.Unflatten(1, (1, -1)),            # Reshape from (batch, L) to (batch, 1, L)\n","            nn.AdaptiveAvgPool1d(1000),            # Pool to fixed size: (batch, 1, 1000)\n","            nn.Flatten(1),                        # Flatten to (batch, 1000)\n","            nn.ReLU(),\n","            nn.Linear(1000, 10)  # Fixed output size (e.g., 10 classes)), ## hier code\n","        )\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        logits = self.linear_relu_stack(x)\n","        return logits\n","\n","model = NeuralNetwork()"],"metadata":{"id":"IdWjs81mu9GV","executionInfo":{"status":"ok","timestamp":1741080314056,"user_tz":-60,"elapsed":8,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["model_output = model(Image1)\n","model_output2 = model(Image2)\n","model_output3 = model(Image3)\n","model_output4 = model(Image4)\n","print(f'Image1.shape {Image1.shape}')\n","print(f'model_output.shape {model_output.shape}')\n","print(f'model_output {model_output}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aDsZma-sUFC4","executionInfo":{"status":"ok","timestamp":1741080457308,"user_tz":-60,"elapsed":11,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}},"outputId":"2092a4cf-cd7a-4630-cb1e-0b1807b373e9"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Image1.shape torch.Size([3, 255, 255])\n","model_output.shape torch.Size([3, 10])\n","model_output tensor([[ 0.0318,  0.0156,  0.2551,  0.4103, -0.3187, -0.2578,  0.0114,  0.6237,\n","         -0.6861,  0.1627],\n","        [ 0.0493, -0.0058,  0.2523,  0.3916, -0.3172, -0.2374,  0.0905,  0.6276,\n","         -0.6725,  0.1251],\n","        [-0.0027,  0.0277,  0.2663,  0.4050, -0.3184, -0.2709,  0.0282,  0.5836,\n","         -0.7069,  0.1151]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"code","source":["model_output = model(Image1)\n","print(\"Original shape of 1st image:\", torch.Size([3, 255, 255]), \"Network Output shape: \", torch.Size([3, 10]))\n"],"metadata":{"id":"_zp3GgGUvd02","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741078456141,"user_tz":-60,"elapsed":265,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}},"outputId":"a1faf4c3-5c17-4c0e-9104-e71f4f680e81"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Original shape of 1st image: torch.Size([3, 255, 255]) Network Output shape:  torch.Size([3, 10])\n"]}]},{"cell_type":"markdown","source":["### 1.2 Sie wollen ein **Dense-Network** erstellen, das auf solchen unterschiedlich grossen Bildern trainiert werden kann **12 Punkte**\n","1. Ergänzen Sie die fehlenden Code-Zeilen **10 Punkte**\n","2. Bestimmen Sie das Output-Shape des Netzwerks **2 Punkte**"],"metadata":{"id":"IXoLhuHO0DtE"}},{"cell_type":"markdown","source":["## 2 Dense-Network Shapes **20 Punkte**\n","\n","\n","---\n","\n"],"metadata":{"id":"k_2vjS6YCuKM"}},{"cell_type":"markdown","source":["### 2.1 Entsprechend der `forward`-Methode, die die Verbindungen zwischen den Sub-Networks festlegt, definieren Sie alle Shapes in der FCN Klasse **12 Punkte**\n","> Die Shapes sind frei wählbar, müssen nur aufeinander passen\n","\n","1. net1  **3 Punkte**\n","2. net2 shapes **3 Punkte**\n","3. net3 shapes **3 Punkte**\n","4. net4 shapes **3 Punkte**\n"],"metadata":{"id":"oqLvgq1kxGI1"}},{"cell_type":"code","source":["# net1 und net2 = input x\n","# net3 = output net1 + net2, aber die Grösse ändert nicht!\n","# net4 = output von net3\n","\n","# x = 128\n","# net1 in: 128, out 32\n","# net2 in: 128, out 32\n","# net3 in: 32, out 8\n","# net4 in: 8, out 1\n","\n","import torch.nn.functional as F\n","class FCN(nn.Module):\n","  def __init__(self):\n","    super(FCN, self).__init__()\n","    self.net1 = nn.Sequential(\n","        nn.Linear(128, 64),  # First layer takes input of size 128 ## hier code\n","        nn.ReLU(),\n","        nn.Linear(64, 32),   # Output size of net1 is 32 ## hier code\n","        nn.ReLU()\n","    )\n","\n","\n","    self.net2 = nn.Sequential(\n","        nn.Linear(128, 64),  # Same input size as net1 ## hier code\n","        nn.ReLU(),\n","        nn.Linear(64, 32),   # Output size of net2 is 32 (same as net1 for addition) ## hier code\n","        nn.ReLU()\n","    )\n","\n","\n","    self.net3 = nn.Sequential(\n","        nn.Linear(32, 16),   # Input is sum of net1 and net2 outputs (both 32) ## hier code\n","        nn.ReLU(),\n","        nn.Linear(16, 8),    # Output size of net3 is 8## hier code\n","        nn.ReLU()\n","\n","    )\n","\n","    self.net4 = nn.Sequential(\n","        nn.Linear(8, 4),     # Input is output of net3 (size 8) ## hier code\n","        nn.ReLU(),\n","        nn.Linear(4, 1)      # Final output size is 1\n","    )\n","\n","\n","  def forward(self, x):\n","    x1 = self.net1(x)\n","    x2 = self.net2(x)\n","    x3 = self.net3(x1 + x2)\n","    return self.net4(x3)"],"metadata":{"id":"QF9wiHI14_2L","executionInfo":{"status":"ok","timestamp":1741078474365,"user_tz":-60,"elapsed":12,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["### 2.2 Erstellen Sie die Input Daten mit 10.000 Zeilen für das FCN Model mit der `torch.rand()` Methode **2 Punkte**"],"metadata":{"id":"F4aLBVKOym1g"}},{"cell_type":"code","source":["model_input = torch.rand((10000, 128))  # 10,000 examples with 128 features each  ## hier code"],"metadata":{"id":"RgqEmBcgDMtj","executionInfo":{"status":"ok","timestamp":1741078478088,"user_tz":-60,"elapsed":21,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["### 2.3 Erstellen Sie das Model Objekt `fcn_model` und berechnen Sie das Model-Output `yhat` **6 Punkte**\n","1. `fcn_model` **3 Punkte**\n","2. `yhat` **3 Punkte**"],"metadata":{"id":"GDfpYT4dzHDr"}},{"cell_type":"code","source":["fcn_model = FCN()## hier code\n","yhat = fcn_model(model_input) ## hier code\n","print(model_input.shape)\n","print(yhat.shape)\n","print(yhat)"],"metadata":{"id":"1aQZ8A7gEJlS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741078609695,"user_tz":-60,"elapsed":30,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}},"outputId":"1e778c37-9160-43c3-ee51-437927f9962c"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10000, 128])\n","torch.Size([10000, 1])\n","tensor([[0.4897],\n","        [0.4894],\n","        [0.4895],\n","        ...,\n","        [0.4895],\n","        [0.4896],\n","        [0.4893]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"code","source":["yhat[-10:]"],"metadata":{"id":"xeNxkc5A-Vit","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741080577795,"user_tz":-60,"elapsed":42,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}},"outputId":"bac93242-a764-4d8c-b3d7-71cfef47b48b"},"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.4896],\n","        [0.4896],\n","        [0.4896],\n","        [0.4896],\n","        [0.4894],\n","        [0.4896],\n","        [0.4894],\n","        [0.4895],\n","        [0.4896],\n","        [0.4893]], grad_fn=<SliceBackward0>)"]},"metadata":{},"execution_count":36}]},{"cell_type":"markdown","source":["## 3 Deep Reinforcement Learning **50 Punkte**\n","\n","Bei dieser Aufgabe werden die allgemeinen Verständnis-Fragen gestellen. Sie dürfen den ChatGPT zwar konslutieren, die Antwort muss jedoch komplett eigenständig geschrieben werden - kein copy & paste (sonst 0 Punkte). Schreiben Sie möglichst kurz, einfach und verständlich, und **nur mit eigenen Worten**.\n","\n","---\n","\n"],"metadata":{"id":"FaeRT9ptngE6"}},{"cell_type":"markdown","source":["## 3.1 On- und off-policy training 20 Punkte\n","1. Definieren Sie beide Training-Strategien und erklären Sie den Unterschied - **5 Punkte**\n","\n","\n","2. Nennen Sie jeweils 1 DRL Verfahren für on- und off-policy training - **5 Punkte**\n","\n","3. Was sind die Nachteile von **on-policy** training (mindestens wesentlicher 1 Nachteil) - **5 Punkte**\n"],"metadata":{"id":"kmbuV3uDJH_Q"}},{"cell_type":"markdown","source":["## Antworten\n","\n","1. Definieren Sie beide Training-Strategien und erklären Sie den Unterschied - **5 Punkte**\n","\n","\n","On Policy: Es wird direkt auf der Policy trainiert, somit kommen die Werte nur von der aktuellen Policy. Die Werte vom Agenten, welche von der aktuellen Policy geholt werden, werden mittels Policy Gradient Methode genutzt, um die Policy zu trainieren. Danach müssen wieder neue Werte vom Agent generiert werden.\n","\n","Off Policy: Die Policy wird mit Werten trainiert, die von einer anderen Policy stammen können oder auch von alten Werten. Diese Werte werden in einem Experience Replay Buffer gesammelt und dann davon gesampelt. Dies ermöglicht, dass die Werte nicht von einander abhängig sind, was für das Training schlecht wäre.\n","\n","Vorteil On-Policy: Somit kann stabil trainiert werden, da immer nur die aktuelle Policy genutzt wird und somit die Werte nicht sehr weit auseinander driften. Bei Off-Policy kommen alte Werte oder falsche Werte vor, welche das Training instabiler machen\n","\n","Vorteil Off-Policy: Werte können mehrfach genutzt werden, somit ist das Training effizienter.\n","\n","\n","2. Nennen Sie jeweils 1 DRL Verfahren für on- und off-policy training - **5 Punkte**\n","\n","* On-Policy: PPO\n","* Off-Policy: DQN\n","\n","3. Was sind die Nachteile von **on-policy** training (mindestens wesentlicher 1 Nachteil) - **5 Punkte**\n","\n","Vorteil Off-Policy: Werte können mehrfach genutzt werden, somit ist das Training effizienter. Dies ist bei On-Policy nicht möglich, es kann immer nur mit dem aktuellen Wert der zu trainierenden Policy gearbeitet werden."],"metadata":{"id":"-rI1TrNXKaMc"}},{"cell_type":"markdown","source":["## 3.2 Experience Replay Buffer (ERB) 15 Punkte\n","1. Zu welchem Zweck wird ERB verwendet?  - **5 Punkte**\n","\n","\n","2. Bei welchen DRL Verfahren darf ERB **NICHT** verwendet werden und warum? - **5 Punkte**\n","\n","\n","3. Welche Auswirkung hat ein kleiner ERB in Bezug auf on-/off-policy Training-Strategie? - **5 Punkte**\n","\n","\n","4. Wenn ein ERB nicht verwendet werden darf, wie kann man trotzdem für sorgen, dass die Trainingsdaten unabhängig und vielfältig sind? - **5 Punkte**\n"],"metadata":{"id":"c9MEDK69KkvF"}},{"cell_type":"markdown","source":["## Antworten\n","\n","1. Zu welchem Zweck wird ERB verwendet?  - **5 Punkte**\n","\n","Der ERB wird verwendet um Erfahrungen (State, ActionA und der Reward) zu sammeln. Bei einer Off-Policy kann dann von diesem ESB gesampelt werden um das Netzwerk zu trainieren. Das Sampeln wird zufällig gemacht, damit die Werte unäbhängig sind, ansonsten könnte das Modell overfitten.\n","\n","2. Bei welchen DRL Verfahren darf ERB **NICHT** verwendet werden und warum? - **5 Punkte**\n","\n","Bei On-Policy Verahren darf ERB nicht verwendet werden, da die Werte für das Training vom aktuellen Netwerk kommen müssen. Ansonsten würde man den Vorteil von On-Policy verlieren und hätte wieder Off-Policy. Bei On-Policy soll gerade das aktuelle Model verwendet werden, damit die Werte nicht zu sehr wegdriften, dies macht das Training stabiler.\n","\n","3. Welche Auswirkung hat ein kleiner ERB in Bezug auf on-/off-policy Training-Strategie? - **5 Punkte**\n","\n","Ein kleiner Buffer hat das Problem, dass beim Training nicht genug unterschiedliche Erfahrung vorhanden sind. Somit lernt das Netzwerk nicht genügend, weil es nicht genügend unterschiedliche Informationen kriegt. Zudem besteht die Gefahr von Oversampling, da er nur wenige Daten kriegt und sich damit genau auf diese Daten einschiesst. Und zuletzt ist nicht nur vieles nicht abgedeckt, die Trainingsdaten sind auch abhängig zueinander. Dies führt zu einem schlechten Training.\n","\n","Dies betrifft nur Off-Policy Training. On-Policy Training benutzt keinen ERB.\n","\n","\n","4. Wenn ein ERB nicht verwendet werden darf, wie kann man trotzdem für sorgen, dass die Trainingsdaten unabhängig und vielfältig sind? - **5 Punkte**\n","\n","Indem man mehrere Umgebungen gleichzeitig nutzt. Somit erhält das Netzwerk unabhängige Daten, da die Umgebungen unabhängig sind."],"metadata":{"id":"GjGS7tLwQdpw"}},{"cell_type":"markdown","source":["## 3.3 Temporal Difference 15 Punkte\n","1. Nennen Sie zwei Methoden, bei denen diese Technik verwendet wird?  - **2 Punkte**\n","\n","\n","2. Erklären Sie mit eigenen Worten, wie Sie diese Methode verstehen - **13 Punkte**\n","> Zum Beispiel: Wie lernt das Model? Was wird im Zuge des Lernens reduziert? Nennen Sie ein einfaches Bespiel aus dem Alltag zur Illustration, wie wir mit der Zeit lernen, den Erignisse, eine richtige Einschätzung zu geben.\n"],"metadata":{"id":"Xr24tq0uOqKq"}},{"cell_type":"markdown","source":["## Antworten\n","\n","1. Nennen Sie zwei Methoden, bei denen diese Technik verwendet wird?  - **2 Punkte**\n","\n","* DQN\n","* A2C\n","\n","2. Erklären Sie mit eigenen Worten, wie Sie diese Methode verstehen - **13 Punkte**\n","> Zum Beispiel: Wie lernt das Model? Was wird im Zuge des Lernens reduziert? Nennen Sie ein einfaches Bespiel aus dem Alltag zur Illustration, wie wir mit der Zeit lernen, den Erignisse, eine richtige Einschätzung zu geben.\n","\n","Man hat 2 Netzwerke, ein aktuelles und eines, welches ein wenig älter ist. Ein Model trainiert (das aktuelle), das andere wird nicht trainiert. Der Fehler zwischen der aktuellen Schätzung der neuen Schätzung ist die Termporale Difference. Das Ziel ist, den Fehler (die zeitliche Differenz) zu minimieren.\n","\n","Dies hat den Vorteil, dass man nicht schon alle Schritte kennen muss, man kann inkrementell lernen.\n","\n","Dies kann Verglichen werden, mit den agilen Methoden. Man versucht etwas (Exploration) und schaut dann zurück, wurde esbesser (das behält man) oder wurde es schlechter (dann korrigiert man). Und dass wiederholt man immer und immer wieder, weil man das Ende gar noch nicht sehen kann. Genau so, funktioniert das lernen mit der Temporal Difference.\n"],"metadata":{"id":"II-aMn0wQegD"}},{"cell_type":"code","source":["print(\"Good Luck!\")"],"metadata":{"id":"mLJ3O-QG5SnN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KIfPj-n3IE9b"},"execution_count":null,"outputs":[]}]}
{"cells":[{"cell_type":"code","execution_count":30,"metadata":{"id":"9u8LXAOLcvqa","executionInfo":{"status":"ok","timestamp":1732626388078,"user_tz":-60,"elapsed":11173,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}},"outputId":"b85a16ba-fbd5-4d41-95c6-f086f4b9998e","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'notebooks'...\n","remote: Enumerating objects: 530, done.\u001b[K\n","remote: Counting objects: 100% (212/212), done.\u001b[K\n","remote: Compressing objects: 100% (53/53), done.\u001b[K\n","remote: Total 530 (delta 183), reused 161 (delta 159), pack-reused 318 (from 1)\u001b[K\n","Receiving objects: 100% (530/530), 30.79 MiB | 20.74 MiB/s, done.\n","Resolving deltas: 100% (252/252), done.\n","/content/notebooks/notebooks\n","⏳ Installing base requirements ...\n","✅ Base requirements installed!\n","⏳ Installing Git LFS ...\n","✅ Git LFS installed!\n"]}],"source":["# Uncomment and run this cell if you're on Colab or Kaggle\n","!git clone https://github.com/nlp-with-transformers/notebooks.git\n","%cd notebooks\n","from install import *\n","install_requirements()"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"hKCElhbTcvqd","executionInfo":{"status":"ok","timestamp":1732626388079,"user_tz":-60,"elapsed":7,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}},"outputId":"6b509aa4-7345-45d2-a6c5-277097f1c6ad","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["No GPU was detected! This notebook can be *very* slow without a GPU 🐢\n","Go to Runtime > Change runtime type and select a GPU hardware accelerator.\n","Using transformers v4.46.2\n","Using datasets v1.16.1\n"]}],"source":["#hide\n","from utils import *\n","setup_chapter()"]},{"cell_type":"markdown","metadata":{"id":"kVSxT6Tzcvqe"},"source":["# Transformer Anatomy"]},{"cell_type":"markdown","metadata":{"id":"JVnSbp9Gcvqf"},"source":["## The Transformer Architecture"]},{"cell_type":"markdown","metadata":{"id":"GujlzdVUcvqf"},"source":["<img alt=\"transformer-encoder-decoder\" caption=\"Encoder-decoder architecture of the transformer, with the encoder shown in the upper half of the figure and the decoder in the lower half\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter03_transformer-encoder-decoder.png?raw=1\" id=\"transformer-encoder-decoder\"/>"]},{"cell_type":"markdown","metadata":{"id":"tavUTKkTcvqf"},"source":["## The Encoder"]},{"cell_type":"markdown","metadata":{"id":"E842imNYcvqf"},"source":["<img alt=\"encoder-zoom\" caption=\"Zooming into the encoder layer\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter03_encoder-zoom.png?raw=1\" id=\"encoder-zoom\"/>"]},{"cell_type":"markdown","metadata":{"id":"W5WRDskWcvqg"},"source":["### Self-Attention"]},{"cell_type":"markdown","metadata":{"id":"JSaGTN-jcvqg"},"source":["<img alt=\"Contextualized embeddings\" caption=\"Diagram showing how self-attention updates raw token embeddings (upper) into contextualized embeddings (lower) to create representations that incorporate information from the whole sequence\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter03_contextualized-embedding.png?raw=1\" id=\"contextualized-embeddings\"/>"]},{"cell_type":"markdown","metadata":{"id":"hIzi79KEcvqg"},"source":["#### Scaled dot-product attention"]},{"cell_type":"markdown","metadata":{"id":"ZSBFC-I2cvqg"},"source":["#hide\n","\n","Copy and execute the following cell magic in a new cell to use `bertviz` in JupyterLab:\n","\n","```python\n","%%javascript\n","require.config({\n","  paths: {\n","      d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.8/d3.min',\n","      jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n","  }\n","});\n","```"]},{"cell_type":"code","source":["%%javascript\n","require.config({\n","  paths: {\n","      d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.8/d3.min',\n","      jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n","  }\n","});"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"CzgbQnoeeBZF","executionInfo":{"status":"ok","timestamp":1732626388079,"user_tz":-60,"elapsed":6,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}},"outputId":"6b119f99-8394-45de-8157-a9747c80da45"},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["require.config({\n","  paths: {\n","      d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.8/d3.min',\n","      jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n","  }\n","});\n"]},"metadata":{}}]},{"cell_type":"code","execution_count":33,"metadata":{"id":"Lh3nQOPEcvqh","outputId":"32d5e755-c169-4cab-92af-f779952e155f","colab":{"base_uri":"https://localhost:8080/","height":393,"output_embedded_package_id":"15HkAHkpa6lvJUVTAIpqKt1f4hvRsWaES"},"executionInfo":{"status":"ok","timestamp":1732626397333,"user_tz":-60,"elapsed":9259,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["#hide_output\n","from transformers import AutoTokenizer\n","from bertviz.transformers_neuron_view import BertModel\n","from bertviz.neuron_view import show\n","\n","model_ckpt = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n","model = BertModel.from_pretrained(model_ckpt)\n","text = \"time flies like an arrow\"\n","show(model, \"bert\", tokenizer, text, display_mode=\"light\", layer=0, head=8)"]},{"cell_type":"markdown","metadata":{"id":"o_ZVJNdAcvqh"},"source":["### Sidebar: Demystifying Queries, Keys, and Values"]},{"cell_type":"markdown","metadata":{"id":"evyOaPiUcvqh"},"source":["### End sidebar"]},{"cell_type":"markdown","metadata":{"id":"zqURpLEfcvqh"},"source":["<img alt=\"Operations in scaled dot-product attention\" height=\"125\" caption=\"Operations in scaled dot-product attention\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter03_attention-ops.png?raw=1\" id=\"attention-ops\"/>"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"4KW-ULvBcvqh","executionInfo":{"status":"ok","timestamp":1732626397333,"user_tz":-60,"elapsed":7,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}}},"outputs":[],"source":["# hide\n","from transformers import AutoTokenizer\n","model_ckpt = \"bert-base-uncased\"\n","text = \"time flies like an arrow\"\n","tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"_X9si1u_cvqh","outputId":"4d67a3a4-9a37-4f39-ff1d-bebbb2b7c045","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732626397333,"user_tz":-60,"elapsed":6,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 2051, 10029,  2066,  2019,  8612]])"]},"metadata":{},"execution_count":35}],"source":["inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n","inputs.input_ids"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"Yw0zFo3rcvqi","outputId":"8c0c4d83-1291-4c3f-df92-0e842fa82e50","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732626397587,"user_tz":-60,"elapsed":258,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Embedding(30522, 768)"]},"metadata":{},"execution_count":36}],"source":["from torch import nn\n","from transformers import AutoConfig\n","\n","config = AutoConfig.from_pretrained(model_ckpt)\n","token_emb = nn.Embedding(config.vocab_size, config.hidden_size)\n","token_emb"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"LqNkazDicvqi","outputId":"e8c1853e-4843-4aa9-cec8-798c3d9bd43f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732626397587,"user_tz":-60,"elapsed":11,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 768])"]},"metadata":{},"execution_count":37}],"source":["inputs_embeds = token_emb(inputs.input_ids)\n","inputs_embeds.size()"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"LuQWssaZcvqi","outputId":"96487ea1-8524-4550-fe87-07f72042f305","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732626397587,"user_tz":-60,"elapsed":9,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 5])"]},"metadata":{},"execution_count":38}],"source":["import torch\n","from math import sqrt\n","\n","query = key = value = inputs_embeds\n","dim_k = key.size(-1)\n","scores = torch.bmm(query, key.transpose(1,2)) / sqrt(dim_k)\n","scores.size()"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"KfeFcuXEcvqi","outputId":"12b0b3a8-a2b9-49db-bad1-5a2be110209e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732626397587,"user_tz":-60,"elapsed":7,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 1., 1., 1., 1.]], grad_fn=<SumBackward1>)"]},"metadata":{},"execution_count":39}],"source":["import torch.nn.functional as F\n","\n","weights = F.softmax(scores, dim=-1)\n","weights.sum(dim=-1)"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"_H12WjgDcvqi","outputId":"b161a2a0-5e5f-40f6-f776-85fcf7c6459e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732626397587,"user_tz":-60,"elapsed":6,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 768])"]},"metadata":{},"execution_count":40}],"source":["attn_outputs = torch.bmm(weights, value)\n","attn_outputs.shape"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"ccBnIECMcvqj","executionInfo":{"status":"ok","timestamp":1732626397587,"user_tz":-60,"elapsed":4,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}}},"outputs":[],"source":["def scaled_dot_product_attention(query, key, value):\n","    dim_k = query.size(-1)\n","    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n","    weights = F.softmax(scores, dim=-1)\n","    return torch.bmm(weights, value)"]},{"cell_type":"markdown","metadata":{"id":"LzSCEVxAcvqj"},"source":["#### Multi-headed attention"]},{"cell_type":"markdown","metadata":{"id":"3UN-2Vpjcvqj"},"source":["<img alt=\"Multi-head attention\" height=\"125\" caption=\"Multi-head attention\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter03_multihead-attention.png?raw=1\" id=\"multihead-attention\"/>"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"qcSoQ9pFcvqj","executionInfo":{"status":"ok","timestamp":1732626397588,"user_tz":-60,"elapsed":5,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}}},"outputs":[],"source":["class AttentionHead(nn.Module):\n","    def __init__(self, embed_dim, head_dim):\n","        super().__init__()\n","        self.q = nn.Linear(embed_dim, head_dim)\n","        self.k = nn.Linear(embed_dim, head_dim)\n","        self.v = nn.Linear(embed_dim, head_dim)\n","\n","    def forward(self, hidden_state):\n","        attn_outputs = scaled_dot_product_attention(\n","            self.q(hidden_state), self.k(hidden_state), self.v(hidden_state))\n","        return attn_outputs"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"QGVkbkXhcvqj","executionInfo":{"status":"ok","timestamp":1732626397588,"user_tz":-60,"elapsed":5,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}}},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        embed_dim = config.hidden_size\n","        num_heads = config.num_attention_heads\n","        head_dim = embed_dim // num_heads\n","        self.heads = nn.ModuleList(\n","            [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]\n","        )\n","        self.output_linear = nn.Linear(embed_dim, embed_dim)\n","\n","    def forward(self, hidden_state):\n","        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)\n","        x = self.output_linear(x)\n","        return x"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"BEF9u0S2cvqj","outputId":"ffd04abe-b6e4-4e30-bddf-8d485faaf941","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732626397588,"user_tz":-60,"elapsed":5,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 768])"]},"metadata":{},"execution_count":44}],"source":["multihead_attn = MultiHeadAttention(config)\n","attn_output = multihead_attn(inputs_embeds)\n","attn_output.size()"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"PxL1k17Ycvqj","outputId":"39a4ce3b-e77a-46bf-e0eb-738ca36a668d","colab":{"base_uri":"https://localhost:8080/","height":442},"executionInfo":{"status":"error","timestamp":1732626400018,"user_tz":-60,"elapsed":220,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}}},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'transformers.models.deprecated.qdqbert'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-45-4414badcc236>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ckpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msentence_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"time flies like an arrow\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0m_LazyAutoMapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m     \"\"\"\n\u001b[1;32m    545\u001b[0m     \u001b[0;31m\"\u001b[0m \u001b[0mA\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0mto\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mload\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0maccessed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mkeys\u001b[0;34m(self)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m_load_attr_from_module\u001b[0;34m(self, model_type, attr)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers.models.deprecated.qdqbert'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["#hide_output\n","from bertviz import head_view\n","from transformers import AutoModel\n","\n","model = AutoModel.from_pretrained(model_ckpt, output_attentions=True)\n","\n","sentence_a = \"time flies like an arrow\"\n","sentence_b = \"fruit flies like a banana\"\n","\n","viz_inputs = tokenizer(sentence_a, sentence_b, return_tensors='pt')\n","attention = model(**viz_inputs).attentions\n","sentence_b_start = (viz_inputs.token_type_ids == 0).sum(dim=1)\n","tokens = tokenizer.convert_ids_to_tokens(viz_inputs.input_ids[0])\n","\n","head_view(attention, tokens, sentence_b_start, heads=[8])"]},{"cell_type":"markdown","metadata":{"id":"8XAW9avNcvqj"},"source":["### The Feed-Forward Layer"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"Labdpl62cvqj","executionInfo":{"status":"ok","timestamp":1732626502111,"user_tz":-60,"elapsed":228,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}}},"outputs":[],"source":["class FeedForward(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n","        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n","        self.gelu = nn.GELU()\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","\n","    def forward(self, x):\n","        x = self.linear_1(x)\n","        x = self.gelu(x)\n","        x = self.linear_2(x)\n","        x = self.dropout(x)\n","        return x"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"ou_kF9ZWcvqk","outputId":"665409c3-9975-42d8-af9a-076d72ced386","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732626504803,"user_tz":-60,"elapsed":212,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 768])"]},"metadata":{},"execution_count":47}],"source":["feed_forward = FeedForward(config)\n","ff_outputs = feed_forward(attn_outputs)\n","ff_outputs.size()"]},{"cell_type":"markdown","metadata":{"id":"2M77VoDmcvqk"},"source":["### Adding Layer Normalization"]},{"cell_type":"markdown","metadata":{"id":"0zaVyU-7cvqk"},"source":["<img alt=\"Transformer layer normalization\" height=\"500\" caption=\"Different arrangements of layer normalization in a transformer encoder layer\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter03_layer-norm.png?raw=1\" id=\"layer-norm\"/>"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"G5tDnGnncvqk","executionInfo":{"status":"ok","timestamp":1732626618361,"user_tz":-60,"elapsed":318,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}}},"outputs":[],"source":["class TransformerEncoderLayer(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n","        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n","        self.attention = MultiHeadAttention(config)\n","        self.feed_forward = FeedForward(config)\n","\n","    def forward(self, x):\n","        # Apply layer normalization and then copy input into query, key, value\n","        hidden_state = self.layer_norm_1(x)\n","        # Apply attention with a skip connection\n","        x = x + self.attention(hidden_state)\n","        # Apply feed-forward layer with a skip connection\n","        x = x + self.feed_forward(self.layer_norm_2(x))\n","        return x"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"lJ1U6Wh-cvqk","outputId":"0f454367-393e-4e39-9ddf-3104623d0492","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732626621250,"user_tz":-60,"elapsed":199,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([1, 5, 768]), torch.Size([1, 5, 768]))"]},"metadata":{},"execution_count":49}],"source":["encoder_layer = TransformerEncoderLayer(config)\n","inputs_embeds.shape, encoder_layer(inputs_embeds).size()"]},{"cell_type":"markdown","metadata":{"id":"u6Ss5cCocvqk"},"source":["### Positional Embeddings"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"QldcKxKWcvqk","executionInfo":{"status":"ok","timestamp":1732626624734,"user_tz":-60,"elapsed":207,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}}},"outputs":[],"source":["class Embeddings(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.token_embeddings = nn.Embedding(config.vocab_size,\n","                                             config.hidden_size)\n","        self.position_embeddings = nn.Embedding(config.max_position_embeddings,\n","                                                config.hidden_size)\n","        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n","        self.dropout = nn.Dropout()\n","\n","    def forward(self, input_ids):\n","        # Create position IDs for input sequence\n","        seq_length = input_ids.size(1)\n","        position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)\n","        # Create token and position embeddings\n","        token_embeddings = self.token_embeddings(input_ids)\n","        position_embeddings = self.position_embeddings(position_ids)\n","        # Combine token and position embeddings\n","        embeddings = token_embeddings + position_embeddings\n","        embeddings = self.layer_norm(embeddings)\n","        embeddings = self.dropout(embeddings)\n","        return embeddings"]},{"cell_type":"code","execution_count":59,"metadata":{"id":"FGxj0V6Tcvql","outputId":"d16f6c35-f16f-47bf-bf4c-f4258542191b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732627019874,"user_tz":-60,"elapsed":236,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 768])"]},"metadata":{},"execution_count":59}],"source":["embedding_layer = Embeddings(config)\n","embedding_layer(inputs.input_ids).size()"]},{"cell_type":"code","execution_count":60,"metadata":{"id":"ZOWsa9Cecvqo","executionInfo":{"status":"ok","timestamp":1732627039850,"user_tz":-60,"elapsed":225,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}}},"outputs":[],"source":["class TransformerEncoder(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.embeddings = Embeddings(config)\n","        self.layers = nn.ModuleList([TransformerEncoderLayer(config)\n","                                     for _ in range(config.num_hidden_layers)])\n","\n","    def forward(self, x):\n","        x = self.embeddings(x)\n","        for layer in self.layers:\n","            x = layer(x)\n","        return x"]},{"cell_type":"code","execution_count":61,"metadata":{"id":"90O42p7tcvqo","outputId":"33163cc0-1065-443c-fcf7-0df267a5e413","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732627050296,"user_tz":-60,"elapsed":1703,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 768])"]},"metadata":{},"execution_count":61}],"source":["encoder = TransformerEncoder(config)\n","encoder(inputs.input_ids).size()"]},{"cell_type":"markdown","metadata":{"id":"EI1mSdiPcvqo"},"source":["### Adding a Classification Head"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"L5jy_fZocvqo","executionInfo":{"status":"ok","timestamp":1732626726562,"user_tz":-60,"elapsed":210,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}}},"outputs":[],"source":["class TransformerForSequenceClassification(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.encoder = TransformerEncoder(config)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","\n","    def forward(self, x):\n","        x = self.encoder(x)[:, 0, :] # select hidden state of [CLS] token\n","        x = self.dropout(x)\n","        x = self.classifier(x)\n","        return x"]},{"cell_type":"code","execution_count":55,"metadata":{"id":"WPRZMzCbcvqo","outputId":"dfc5a480-c962-4b66-b700-e77291b78865","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732626730715,"user_tz":-60,"elapsed":2090,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 3])"]},"metadata":{},"execution_count":55}],"source":["config.num_labels = 3\n","encoder_classifier = TransformerForSequenceClassification(config)\n","encoder_classifier(inputs.input_ids).size()"]},{"cell_type":"markdown","metadata":{"id":"9MDHnG_jcvqp"},"source":["## The Decoder"]},{"cell_type":"markdown","metadata":{"id":"xdkwQ_cpcvqp"},"source":["<img alt=\"Transformer decoder zoom\" caption=\"Zooming into the transformer decoder layer\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter03_decoder-zoom.png?raw=1\" id=\"decoder-zoom\"/>"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"43FIIXMBcvqp","outputId":"c6fa4056-50a3-4de4-b9df-5b19bbef0f1f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732626737121,"user_tz":-60,"elapsed":213,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 0., 0., 0., 0.],\n","        [1., 1., 0., 0., 0.],\n","        [1., 1., 1., 0., 0.],\n","        [1., 1., 1., 1., 0.],\n","        [1., 1., 1., 1., 1.]])"]},"metadata":{},"execution_count":56}],"source":["seq_len = inputs.input_ids.size(-1)\n","mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)\n","mask[0]"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"4cYSyp8hcvqp","outputId":"e69f32a0-2248-448b-cda2-e88ef9ef18c5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732626740388,"user_tz":-60,"elapsed":230,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[28.9860,    -inf,    -inf,    -inf,    -inf],\n","         [ 0.6920, 28.2319,    -inf,    -inf,    -inf],\n","         [ 0.5534, -0.3532, 28.5119,    -inf,    -inf],\n","         [ 1.1536,  0.3130, -0.5945, 26.3529,    -inf],\n","         [ 1.2730, -0.2190,  0.1663,  0.3643, 25.3603]]],\n","       grad_fn=<MaskedFillBackward0>)"]},"metadata":{},"execution_count":57}],"source":["scores.masked_fill(mask == 0, -float(\"inf\"))"]},{"cell_type":"code","execution_count":58,"metadata":{"id":"GjPR6Zfmcvqp","executionInfo":{"status":"ok","timestamp":1732626908442,"user_tz":-60,"elapsed":242,"user":{"displayName":"Roger Briggen","userId":"03435179165522252011"}}},"outputs":[],"source":["def scaled_dot_product_attention(query, key, value, mask=None):\n","    dim_k = query.size(-1)\n","    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n","    if mask is not None:\n","        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n","    weights = F.softmax(scores, dim=-1)\n","    return weights.bmm(value)"]},{"cell_type":"markdown","metadata":{"id":"I5ukvr_bcvqp"},"source":["### Sidebar: Demystifying Encoder-Decoder Attention"]},{"cell_type":"markdown","metadata":{"id":"Ov6qkvG9cvqp"},"source":["### End sidebar"]},{"cell_type":"markdown","metadata":{"id":"1bp584Srcvqp"},"source":["## Meet the Transformers"]},{"cell_type":"markdown","metadata":{"id":"DliXgNuicvqp"},"source":["### The Transformer Tree of Life"]},{"cell_type":"markdown","metadata":{"id":"NcwKPcHccvqq"},"source":["<img alt=\"Transformer family tree\" caption=\"An overview of some of the most prominent transformer architectures\" src=\"https://github.com/nlp-with-transformers/notebooks/blob/main/images/chapter03_transformers-compact.png?raw=1\" id=\"family-tree\"/>"]},{"cell_type":"markdown","metadata":{"id":"HS1o7-Wqcvqq"},"source":["### The Encoder Branch"]},{"cell_type":"markdown","metadata":{"id":"iHbvDORAcvqq"},"source":["### The Decoder Branch"]},{"cell_type":"markdown","metadata":{"id":"9yhiLd9Pcvqq"},"source":["### The Encoder-Decoder Branch"]},{"cell_type":"markdown","metadata":{"id":"TFHCChnNcvqq"},"source":["## Conclusion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ud9CpIw-cvqq"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"colab":{"provenance":[{"file_id":"https://github.com/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb","timestamp":1732624387249}]}},"nbformat":4,"nbformat_minor":0}
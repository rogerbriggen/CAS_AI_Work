{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73cb0a7d",
   "metadata": {},
   "source": [
    "# Aufgabe 5 - RNN\n",
    "\n",
    "1. Train RNN on your images from last homework\n",
    "  - try to vary some hypoparams (number of hidden layers, number neurons)\n",
    "  - color images as minimum grayscale or add the channels or avarage or any creative solution you like\n",
    "2. Optinional but desired: find timeseries of any kind with one variable (e.g. ECG dataset with two labels (normal/not normal or any you find)\n",
    " handle timeseries like images with size (number of timesteps, 1)\n",
    " Take as example https://colab.research.google.com/drive/1tDhcSf4SlEqCJq19_S7lYWHCPoOqDkLH?usp=sharing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b518cece-4fc9-4459-89df-36313148ab9e",
   "metadata": {
    "id": "b518cece-4fc9-4459-89df-36313148ab9e"
   },
   "outputs": [],
   "source": [
    "# We need to be able to convert the data for the images back into something useful\n",
    "import pickle # for serializing and deserializing objects. In simpler terms, it allows you to convert a Python object into a byte stream (so you can save it to a file or send it over a network) and then convert that byte stream back into a Python object.\n",
    "# Useful maths functions\n",
    "import numpy as np # add support for N-dimensional arrays, standard maths functions, linear algebra and all the stuff you sort of remember from school\n",
    "# Pytorch is what this course seems to be about so lets have that one too\n",
    "import torch\n",
    "# DataLoader: efficient way to iterate over datasets in batches. Also supports shuffling and multithreading\n",
    "# Dataset: abstract class for creating ones own datasets. Requires \"__len__\" (size of the dataset) and \"__getitem__\" (sample from dataset)\n",
    "# TensorDataset: If you have a dataset comprised of tensors this wrapper will enable the extraction of tensors as tuples\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, random_split\n",
    "\n",
    "from torch import nn # add neural network functionality as per class instructions\n",
    "from torchvision.transforms import ToTensor, Lambda # needed for some vision work... let's see if we actually need it\n",
    "from torchvision import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "Q5IwqedPm6sI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q5IwqedPm6sI",
    "outputId": "9cf78fd2-f5e7-4fae-9fbc-30e5589ccb3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the PYTORCH_CUDA_ALLOC_CONF environment variable\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "device = 'cpu' # for now we will use the cpu\n",
    "torch.set_default_device(device)  # set the default device to be used for tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "Vz1yCV28MsRe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vz1yCV28MsRe",
    "outputId": "4a37a07f-b31e-478e-e72d-d004b7be8beb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_download_path: g:\\Meine Ablage\\Colab Notebooks\n",
      "dataset_path: data/cifar-10-batches-py\n",
      "files and directories in data: ['cifar-10-batches-py']\n",
      "files and directories in data/cifar-10-batches-py: ['data_batch_4', 'readme.html', 'test_batch', 'data_batch_3', 'batches.meta', 'data_batch_2', 'data_batch_5', 'data_batch_1']\n"
     ]
    }
   ],
   "source": [
    "# Just for test purpose... we do it on our own\n",
    "'''\n",
    "# Fetching the data with the built-in mechanism from pythorch, they can be also fetched from the source https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "# Creating the datasets for training, validation and testing\n",
    "training_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "'''\n",
    "\n",
    "\n",
    "# Fetching the data from the source https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "\n",
    "data_url = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "data_filename = 'cifar-10-python.tar.gz'\n",
    "extracted_data_path = 'cifar-10-batches-py'\n",
    "dataset_basepath = 'data'\n",
    "dataset_path = dataset_basepath + '/' + extracted_data_path\n",
    "dataset_path_for_loader = dataset_path + '/'\n",
    "\n",
    "# get the current working directory... set this to a path where the files will be stored\n",
    "dataset_download_path = os.getcwd()\n",
    "\n",
    "# change the working directory to the dataset_path since the download will download it to the current working directory\n",
    "os.chdir(dataset_download_path)\n",
    "\n",
    "\n",
    "\n",
    "# Let's see what we have\n",
    "print(f\"dataset_download_path: {dataset_download_path}\")\n",
    "\n",
    "\n",
    "# fetch the file (will be saved to the current working path) if not existing\n",
    "if not os.path.isfile(data_filename):\n",
    "    urllib.request.urlretrieve(data_url, data_filename)\n",
    "\n",
    "\n",
    "print(f\"dataset_path: {dataset_path}\")\n",
    "\n",
    "# delete a files and all subdirectories! For testing purpose\n",
    "#shutil.rmtree(dataset_basepath)\n",
    "\n",
    "# extract the files if not already existing\n",
    "if not os.path.isdir(dataset_path):\n",
    "    with tarfile.open(data_filename, 'r:gz') as tar:\n",
    "        tar.extractall(path=dataset_basepath)\n",
    "\n",
    "# List files and directories\n",
    "files = os.listdir(dataset_basepath)\n",
    "print(f\"files and directories in {dataset_basepath}: {files}\")\n",
    "files = os.listdir(dataset_path)\n",
    "print(f\"files and directories in {dataset_path}: {files}\")\n",
    "\n",
    "# make sure we have some files or directories\n",
    "if not os.listdir(dataset_path + \"/\"):\n",
    "    sys.exit(\"Witout files or directories it is useless to proceed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d63e17c-b93b-4611-a756-9174871d0fa6",
   "metadata": {
    "id": "4d63e17c-b93b-4611-a756-9174871d0fa6"
   },
   "outputs": [],
   "source": [
    "# Path to the extracted files\n",
    "DATA_PATH = dataset_path_for_loader\n",
    "\n",
    "# Load a single batch file\n",
    "def load_cifar_batch(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        batch = pickle.load(f, encoding='bytes')\n",
    "        data = batch[b'data']\n",
    "        labels = batch[b'labels']\n",
    "\n",
    "    # Reshape the data to the (num_samples, 3, 32, 32) format\n",
    "    data = data.reshape(-1, 3, 32, 32).astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    data_tensor = torch.tensor(data)\n",
    "    labels_tensor = torch.tensor(labels)\n",
    "    return TensorDataset(data_tensor, labels_tensor)\n",
    "\n",
    "# If you look at the screen shot above, you will see that there are five equally sized data batches and one test batch.\n",
    "# Load data batches 1-4 from their files\n",
    "train_batches = []\n",
    "for i in range(1, 4):\n",
    "    train_batches.append(load_cifar_batch(f'{DATA_PATH}data_batch_{i}'))\n",
    "\n",
    "# Combine the first four data batches into a single dataset for training\n",
    "train_data = torch.utils.data.ConcatDataset(train_batches)\n",
    "\n",
    "# Load data_batch_5 as validation set\n",
    "val_data = load_cifar_batch(f'{DATA_PATH}data_batch_5')\n",
    "\n",
    "# Load test_batch as test set\n",
    "test_data = load_cifar_batch(f'{DATA_PATH}test_batch')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d282d42-2035-4299-818e-372e38ff443a",
   "metadata": {
    "id": "2d282d42-2035-4299-818e-372e38ff443a"
   },
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Define a starting learning rate (hopefully with an optimizer this won't be mission critical)\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Set a small number of epochs to start with\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af840875-2c8a-41c9-9cff-eff6f82355bf",
   "metadata": {
    "id": "af840875-2c8a-41c9-9cff-eff6f82355bf"
   },
   "source": [
    "# 2. Train/Validation/Test split (validation and test - no shuffle) - sklearn or manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76f04995-522d-4bcf-b31d-5ff81a7e1029",
   "metadata": {
    "id": "76f04995-522d-4bcf-b31d-5ff81a7e1029"
   },
   "outputs": [],
   "source": [
    "# Create DataLoader objects\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=device)) # Shuffle training set. Because of shuffle, we need to set the genrator set to the actual device\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False) # Don't shuffle validation set\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False) # Don't shuffle the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4c853e3-cd27-42b8-93eb-0142b6b554b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4c853e3-cd27-42b8-93eb-0142b6b554b9",
    "outputId": "09c58bd6-1f89-4a8c-ad2d-5288626ebb0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001F0E77A5490>\n",
      "torch.Size([16, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "print(train_loader)\n",
    "\n",
    "for X, y in train_loader:\n",
    "    print(X.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e795d6d6-19dd-4de1-8469-59d80c6b660a",
   "metadata": {
    "id": "e795d6d6-19dd-4de1-8469-59d80c6b660a"
   },
   "source": [
    "## Sanity check of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fdba977-c989-4d81-b3fb-bf5369c57f17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fdba977-c989-4d81-b3fb-bf5369c57f17",
    "outputId": "aa595512-f52b-4d40-acc6-f67809ce0688"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# We should check the data look OK and that we understand exactly what we have\n",
    "# Let's plot an image and the corresponding pre-defined classification\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Get the data from the train_loader\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "# Extract the first image and label from the batch\n",
    "# Note, this will be a different image each time you plot it because the act of calling \"train_loader\" will randomise the picture order\n",
    "# Try running this cell multipe times and the label will change...\n",
    "image = images[0]\n",
    "label = labels[0].item()\n",
    "\n",
    "# How are the labels defined in the data set?\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7214417d-a8b8-4594-b41e-11eb2afc6d5f",
   "metadata": {
    "id": "7214417d-a8b8-4594-b41e-11eb2afc6d5f"
   },
   "source": [
    "## Grrr... The label is a zero-based integer not a string...\n",
    "The data can be found here: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "Presumably \"airplane\" is label[0] and \"truck\" is label[9]\n",
    "so..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66d53b95-20fc-424e-990f-04f782f345b4",
   "metadata": {
    "id": "66d53b95-20fc-424e-990f-04f782f345b4"
   },
   "outputs": [],
   "source": [
    "# Define the CIFAR-10 classes for annotation\n",
    "cifar10_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d871f27d-949c-4e13-b2f3-46f2d9b6b207",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "d871f27d-949c-4e13-b2f3-46f2d9b6b207",
    "outputId": "5c7865c6-9695-403c-9d04-ee40e01b43e2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert the image tensor to a NumPy array and transpose to (32, 32, 3) for plotting\n",
    "image_np = image.cpu().permute(1, 2, 0).numpy()\n",
    "\n",
    "# The images were normalised for the \"intended use\", so we need to un-normalise them for plotting\n",
    "image_np = image_np * 255.0\n",
    "\n",
    "# Display the image with its label\n",
    "plt.imshow(image_np.astype(\"uint8\"))\n",
    "plt.title(f\"Label: {cifar10_classes[label]}, Numerical: {label}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b38cccfb",
   "metadata": {
    "id": "b38cccfb"
   },
   "outputs": [],
   "source": [
    "class Result():\n",
    "    def __init__(self):\n",
    "        self.best_val_accuracy = 0 # So we can tell if the new model gets better\n",
    "        self.best_pred = None # holds the best prediction to plot the model graph later\n",
    "        self.epochs_used = 0 # How many epochs were used to train the model\n",
    "\n",
    "        self.train_loss_history = []\n",
    "        self.recall_history = []\n",
    "        self.precision_history = []\n",
    "\n",
    "        # Early stopping proposal is to check in each EPOCH how good the model is with the validation set\n",
    "        # If the validation performs better than any previous Epoch then make the current model the best model and set the patience_counter = 0\n",
    "        # If the validation performs worse than than the best model then increase the patience_counter by +1\n",
    "        # If the patience_counter exceeds the patience threshhold then stop training and use the best model found with the test data set\n",
    "        self.patience, self.patience_counter = 5, 0 # If the model doesn't improve relative to the best model then stop after this main epochs without improvement\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5a0fdc3",
   "metadata": {
    "id": "c5a0fdc3"
   },
   "outputs": [],
   "source": [
    "class Experiment():\n",
    "    def __init__(self, model, optimizer=None, scheduler=None, criterion=None):\n",
    "        self.model = model\n",
    "        self.model_name = model.__class__.__name__\n",
    "        self.model_params = model.get_init_params()\n",
    "        if (optimizer == None):\n",
    "            self.optimizer = self.create_optimizer()\n",
    "        else:\n",
    "            self.optimizer = optimizer\n",
    "        if (scheduler == None):\n",
    "            self.scheduler = self.create_scheduler()\n",
    "        else:\n",
    "            self.scheduler = scheduler\n",
    "        if (criterion == None):\n",
    "            self.criterion = nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            self.criterion = criterion\n",
    "        self.result = Result()\n",
    "\n",
    "    def create_optimizer(self):\n",
    "        # Use ADAM as an optimizer as it seemed to perform pretty well last time\n",
    "        # Maybe compare with other optimizers...\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def create_scheduler(self):\n",
    "        # What is the best learning rate?\n",
    "        # scheduler: This is the learning rate scheduler object that you can use to adjust the learning rate during each epoch or training step.\n",
    "        # torch.optim.lr_scheduler.CosineAnnealingLR: This is a specific type of learning rate scheduler in PyTorch that decreases the learning rate following a cosine curve.\n",
    "        # optimizer: This is the optimizer that the learning rate scheduler will modify. The optimizer manages the parameters and updates them based on the learning rate.\n",
    "        # T_max=10: This parameter specifies the maximum number of iterations (or epochs) before the learning rate reaches its minimum value. In this case, T_max=10 means that over the course of 10 epochs, the learning rate will decrease following a cosine curve until it reaches the minimum.\n",
    "        return torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=10, eta_min=1e-6)\n",
    "    \n",
    "    def free_memory(self):\n",
    "        # Free up memory\n",
    "        del self.model\n",
    "        del self.optimizer\n",
    "        del self.scheduler\n",
    "        del self.criterion\n",
    "        # del self.result // we keep the result for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598b0735-548c-4c38-a4d7-695e70f39460",
   "metadata": {
    "id": "598b0735-548c-4c38-a4d7-695e70f39460"
   },
   "source": [
    "# 3. When training - calculate loss etc on both (train and valid) and decide on training success based on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a20a1b2b-3c7e-4192-bb39-197475c21601",
   "metadata": {
    "id": "a20a1b2b-3c7e-4192-bb39-197475c21601"
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    model_pred = None\n",
    "    for X, y in dataloader: # X is the data and y the label\n",
    "        X, y = X.to(device), y.to(device) # device is either CPU or GPU\n",
    "\n",
    "        optimizer.zero_grad() # We don't want the previous gradients to contaminate the optimizer so we reset them\n",
    "        model_pred = model(X) # data is passed through the model to generate a prediction of the label\n",
    "        loss = loss_fn(model_pred, y) # loss_fn uses the \"cross entropy\" model mentioned in class to compare the predicted label (pred) with the actual lable (y). A low value is good\n",
    "        # Backpropagation: Back propagation is attempting to figure out which of the weights are responsible for most of the error. It does this by calculating the error between the predicted output and actual output and then propagating this error back through the layers to find out how much each weight contributed to the error. Like tuning a radio, the learning rate comes in here and is related to how big a click the weight knob can be turned in one step up or down in order that the overall error can be reduced.\n",
    "        loss.backward() # Calculate the gradients of the loss\n",
    "        optimizer.step() # Tune the radio to reduce the error using Adam to estimate how much to tune it by\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss, model_pred\n",
    "\n",
    "# Validation loop: this can be used for both the validation data during the training and the test data after the best model has been found\n",
    "def validate_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss, correct = 0, 0\n",
    "    pred = None\n",
    "    recall = None\n",
    "    true_positives = 0\n",
    "    false_negatives = 0\n",
    "    false_positives = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "\n",
    "            y_pred = (pred.argmax(1) == 1).float()\n",
    "            true_positives += ((y_pred == 1) & (y == 1)).sum().item()\n",
    "            false_negatives += ((y_pred == 0) & (y == 1)).sum().item()\n",
    "            false_positives += ((y_pred == 1) & (y == 0)).sum().item()\n",
    "\n",
    "            total_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    precision = 0\n",
    "    if (true_positives + false_positives) > 0:\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / len(dataloader.dataset)\n",
    "    return avg_loss, accuracy, pred, recall, precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382c1791-b043-40d9-86c1-df7155be0427",
   "metadata": {
    "id": "382c1791-b043-40d9-86c1-df7155be0427"
   },
   "source": [
    "## 4. Implement your own early stopping: i.e. if validation loss has not decreased over n (to be defined, e.g. 10 or 20) iterations (delta to be defined, e.g. 10^(-4)), training is stopped and the last model is saved - please find out for yourself how to save a torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "096f75e4-1985-4ab3-966a-e1d47355c6a9",
   "metadata": {
    "id": "096f75e4-1985-4ab3-966a-e1d47355c6a9"
   },
   "outputs": [],
   "source": [
    "class model_exec_loop():\n",
    "    def __init__(self, i, experiment):\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=device)) # Shuffle training set. Because of shuffle, we need to set the genrator set to the actual device\n",
    "        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False) # Don't shuffle validation set\n",
    "        test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False) # Don't shuffle the test set\n",
    "        self.experiment = experiment\n",
    "\n",
    "        print(f\"Training model {experiment.model.__class__.__name__}_{experiment.model.get_init_params()} with hidden sizes {experiment.model.get_init_params()} \\n\")\n",
    "        # Loop training \"epochs\" number of times\n",
    "        for t in range(epochs):\n",
    "            print(f\"Epoch {t+1} for {i}\\n-------------------------------\")\n",
    "            # Run the training from task 3\n",
    "            train_loss, model_pred = train_loop(train_loader, experiment.model, experiment.loss_fn, experiment.optimizer)\n",
    "            self.experiment.result.train_loss_history.append(train_loss)\n",
    "            # After each attempt at training test the model against the validation set\n",
    "            val_loss, val_accuracy, pred, recall, precision = validate_loop(val_loader, experiment.model, experiment.loss_fn)\n",
    "            self.experiment.result.recall_history.append(recall)\n",
    "            self.experiment.result.precision_history.append(precision)\n",
    "            print(f\"{i} Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {(100*val_accuracy):>0.1f}%, Recall: {recall}, Precision: {precision}\")\n",
    "\n",
    "            # Check for improvement and if better than before, write out the model to disk\n",
    "            if val_accuracy > self.experiment.result.best_val_accuracy: # New model is better than the old model\n",
    "                self.experiment.result.best_val_accuracy = val_accuracy\n",
    "                self.experiment.result.best_pred = model_pred\n",
    "                torch.save(experiment.model.state_dict(), f\"best_model_{i}.pth\")\n",
    "                self.experiment.result.patience_counter = 0 # I am interested again, reset my boredom levels\n",
    "                print(f\"Best model saved for {i}.\")\n",
    "            else:\n",
    "                # If the model is worse then increase the patience_counter until we give up\n",
    "                self.experiment.result.patience_counter += 1 # Computer gets slightly more bored when the new model is less good than the best so far\n",
    "                if self.experiment.result.patience_counter >= self.experiment.result.patience: # Boredom threshhold is past so the computer gives up as the model isn't improving\n",
    "                    print(f\"Early stopping triggered for {i}. Training stopped. \\n\")\n",
    "                    break\n",
    "\n",
    "            # Adjust learning rate\n",
    "            experiment.scheduler.step() # Slightly unclear how this actually works\n",
    "            print(f\"Learning Rate for {i}: {experiment.scheduler.get_last_lr()[0]} \\n\")\n",
    "\n",
    "        # Load the best model for testing\n",
    "        experiment.model.load_state_dict(torch.load(f\"best_model_{i}.pth\", weights_only=True))\n",
    "        print(\"Testing on the test dataset with the best model:\")\n",
    "        test_loss, test_accuracy, pred, recall, precision = validate_loop(test_loader, experiment.model, experiment.loss_fn)\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {(100*test_accuracy):>0.1f}%, Recall: {recall}, Precision: {precision}\")\n",
    "        print(\"****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43ef672e-b525-4642-b2c5-83723b83d655",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn_exec_loop():\n",
    "    def __init__(self, i, experiment):\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=device)) # Shuffle training set. Because of shuffle, we need to set the genrator set to the actual device\n",
    "        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False) # Don't shuffle validation set\n",
    "        test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False) # Don't shuffle the test set\n",
    "        self.experiment = experiment\n",
    "\n",
    "        print(f\"Training model {experiment.model.get_init_params()} \\n\")\n",
    "        # Loop training \"epochs\" number of times\n",
    "        for t in range(epochs):\n",
    "            print(f\"Epoch {t+1} for {i}\\n-------------------------------\")\n",
    "            # Run the training from task 3\n",
    "            train_loss, model_pred = train_loop(train_loader, experiment.model, experiment.loss_fn, experiment.optimizer)\n",
    "            self.experiment.result.train_loss_history.append(train_loss)\n",
    "            # After each attempt at training test the model against the validation set\n",
    "            val_loss, val_accuracy, pred, recall, precision = validate_loop(val_loader, experiment.model, experiment.loss_fn)\n",
    "            self.experiment.result.recall_history.append(recall)\n",
    "            self.experiment.result.precision_history.append(precision)\n",
    "            print(f\"{i} Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {(100*val_accuracy):>0.1f}%, Recall: {recall}, Precision: {precision}\")\n",
    "\n",
    "            # Check for improvement and if better than before, write out the model to disk\n",
    "            if val_accuracy > self.experiment.result.best_val_accuracy: # New model is better than the old model\n",
    "                self.experiment.result.best_val_accuracy = val_accuracy\n",
    "                self.experiment.result.best_pred = model_pred\n",
    "                torch.save(experiment.model.state_dict(), f\"best_model_{i}.pth\")\n",
    "                self.experiment.result.patience_counter = 0 # I am interested again, reset my boredom levels\n",
    "                print(f\"Best model saved for {i}.\")\n",
    "            else:\n",
    "                # If the model is worse then increase the patience_counter until we give up\n",
    "                self.experiment.result.patience_counter += 1 # Computer gets slightly more bored when the new model is less good than the best so far\n",
    "                if self.experiment.result.patience_counter >= self.experiment.result.patience: # Boredom threshhold is past so the computer gives up as the model isn't improving\n",
    "                    print(f\"Early stopping triggered for {i}. Training stopped. \\n\")\n",
    "                    break\n",
    "\n",
    "            # Adjust learning rate\n",
    "            experiment.scheduler.step() # Slightly unclear how this actually works\n",
    "            print(f\"Learning Rate for {i}: {experiment.scheduler.get_last_lr()[0]} \\n\")\n",
    "\n",
    "        # Load the best model for testing\n",
    "        experiment.model.load_state_dict(torch.load(f\"best_model_{i}.pth\", weights_only=True))\n",
    "        print(\"Testing on the test dataset with the best model:\")\n",
    "        test_loss, test_accuracy, pred, recall, precision = validate_loop(test_loader, experiment.model, experiment.loss_fn)\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {(100*test_accuracy):>0.1f}%, Recall: {recall}, Precision: {precision}\")\n",
    "        print(\"****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0ce3eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "class rnn_exec_loop():\n",
    "    def __init__(self, j, experiment):\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, generator=torch.Generator(device=device)) # Shuffle training set. Because of shuffle, we need to set the genrator set to the actual device\n",
    "        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False) # Don't shuffle validation set\n",
    "        test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False) # Don't shuffle the test set\n",
    "        self.experiment = experiment\n",
    "\n",
    "        \n",
    "\n",
    "        print(f\"Training model {experiment.model.__class__.__name__}_{experiment.model.get_init_params()} with hidden sizes {experiment.model.get_init_params()} \\n\")\n",
    "        # Number of steps to unroll\n",
    "        seq_dim = experiment.model.input_dim * 3 # (3 channels because of the colors)\n",
    "\n",
    "        iter = 0\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch+1} for {j}\\n-------------------------------\")\n",
    "            print(f\"Memory used: {psutil.virtual_memory().percent}%\")\n",
    "            for i, (images, labels) in enumerate(train_loader):\n",
    "                experiment.model.train()\n",
    "                # Load images as tensors with gradient accumulation abilities\n",
    "                images = images.reshape(-1, seq_dim, experiment.model.input_dim).requires_grad_()\n",
    "                #images = images.permute(0, 2, 3, 1).reshape(-1, seq_dim, experiment.model.input_dim).requires_grad_().to(device)\n",
    "\n",
    "                # Clear gradients w.r.t. parameters\n",
    "                experiment.optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass to get output/logits\n",
    "                # outputs.size() --> 100, 10\n",
    "                outputs = experiment.model(images)\n",
    "\n",
    "                # Calculate Loss: softmax --> cross entropy loss\n",
    "                loss = experiment.criterion(outputs, labels)\n",
    "                self.experiment.result.train_loss_history.append(loss)\n",
    "\n",
    "                # Getting gradients w.r.t. parameters\n",
    "                loss.backward()\n",
    "\n",
    "                # Updating parameters\n",
    "                experiment.optimizer.step()\n",
    "\n",
    "                iter += 1\n",
    "                early_stopping = False\n",
    "\n",
    "                if iter % 500 == 0:\n",
    "                    experiment.model.eval()\n",
    "                    # Calculate Accuracy\n",
    "                    correct = 0\n",
    "                    total = 0\n",
    "                    # Iterate through test dataset\n",
    "                    for images, labels in test_loader:\n",
    "                        # Load images to a Torch tensors with gradient accumulation abilities\n",
    "                        images = images.reshape(-1, seq_dim, experiment.model.input_dim)\n",
    "\n",
    "                        # Forward pass only to get logits/output\n",
    "                        outputs = experiment.model(images)\n",
    "\n",
    "                        # Get predictions from the maximum value\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                        # Total number of labels\n",
    "                        total += labels.size(0)\n",
    "\n",
    "                        # Total correct predictions\n",
    "                        correct += (predicted == labels).sum()\n",
    "\n",
    "                    val_accuracy = 100 * correct / total\n",
    "                    if val_accuracy > self.experiment.result.best_val_accuracy: # New model is better than the old model\n",
    "                        self.experiment.result.best_val_accuracy = val_accuracy\n",
    "                        self.experiment.result.best_pred = predicted  # todo: is this correct?\n",
    "                        torch.save(experiment.model.state_dict(), f\"best_model_{j}.pth\")\n",
    "                        self.experiment.result.patience_counter = 0 # I am interested again, reset my boredom levels\n",
    "                        print(f\"Best model saved for {j}.\")\n",
    "                    else:\n",
    "                        # If the model is worse then increase the patience_counter until we give up\n",
    "                        self.experiment.result.patience_counter += 1 # Computer gets slightly more bored when the new model is less good than the best so far\n",
    "                        if self.experiment.result.patience_counter >= self.experiment.result.patience: # Boredom threshhold is past so the computer gives up as the model isn't improving\n",
    "                            print(f\"Early stopping triggered for {j}. Training stopped. \\n\")\n",
    "                            early_stopping = True\n",
    "                    \n",
    "                    # Print Loss\n",
    "                    print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), val_accuracy))\n",
    "\n",
    "                if early_stopping:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cda19f",
   "metadata": {
    "id": "92cda19f"
   },
   "source": [
    "# RNN Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953ab7b5",
   "metadata": {},
   "source": [
    "## Model A: 1 Hidden Layer (ReLU)\n",
    "- Unroll 32 time steps\n",
    "    - Each step input size: 32 x 1\n",
    "    - Total per unroll: 32 x 32*3\n",
    "        - Feedforward Neural Network input size: 32 x 32*3\n",
    "- 1 Hidden layer\n",
    "- ReLU Activation Function\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1GDaMOh5TXnvr8W8xfdghXbbSEyI6M4b1\" width=\"900\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8df403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model A: 1 Hidden Layer ReLU\n",
    "class RNNModel_1HiddenLayer_ReLU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(RNNModel_1HiddenLayer_ReLU, self).__init__()\n",
    "        \n",
    "        # Number of input dim\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # Number of output dimensions\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Building your RNN\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, input_dim)\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu') #\n",
    "\n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        # (layer_dim, batch_size, hidden_dim)\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # We need to detach the hidden state to prevent exploding/vanishing gradients\n",
    "        # This is part of truncated backpropagation through time (BPTT)\n",
    "        out, hn = self.rnn(x, h0.detach())\n",
    "\n",
    "        # Index hidden state of last time step\n",
    "        # out.size() --> 100, 28, 10\n",
    "        # out[:, -1, :] --> 100, 10 --> just want last time step hidden states!\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        # out.size() --> 100, 10\n",
    "        return out\n",
    "    \n",
    "    def get_init_params(self):\n",
    "        return f'input_dim={self.input_dim}, hidden_dim={self.hidden_dim}, layer_dim={self.layer_dim}, output_dim={self.output_dim}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1de5129-f23c-4f09-bec7-596458b8de64",
   "metadata": {},
   "outputs": [],
   "source": [
    "force_rnn_experiment = True # Set to True to force the training of the models again, set to false to skip if already computed\n",
    "if force_rnn_experiment == True:\n",
    "    rnn_experiments = [] # Reset the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79d0ebdb-3b4a-4df0-a36c-6c9548bedf8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model RNNModel_1HiddenLayer_ReLU_input_dim=32, hidden_dim=100, layer_dim=1, output_dim=10 with hidden sizes input_dim=32, hidden_dim=100, layer_dim=1, output_dim=10 \n",
      "\n",
      "Epoch 1 for 1\n",
      "-------------------------------\n",
      "Memory used: 37.4%\n",
      "Best model saved for 1.\n",
      "Iteration: 500. Loss: 2.131485939025879. Accuracy: 19.559999465942383\n",
      "Best model saved for 1.\n",
      "Iteration: 1000. Loss: 2.1129095554351807. Accuracy: 21.030000686645508\n",
      "Best model saved for 1.\n",
      "Iteration: 1500. Loss: 2.2409284114837646. Accuracy: 22.020000457763672\n",
      "Epoch 2 for 1\n",
      "-------------------------------\n",
      "Memory used: 42.9%\n",
      "Iteration: 2000. Loss: 2.205427408218384. Accuracy: 19.540000915527344\n",
      "Best model saved for 1.\n",
      "Iteration: 2500. Loss: 1.987099528312683. Accuracy: 22.540000915527344\n",
      "Best model saved for 1.\n",
      "Iteration: 3000. Loss: 1.8214974403381348. Accuracy: 22.559999465942383\n",
      "Best model saved for 1.\n",
      "Iteration: 3500. Loss: 1.9987901449203491. Accuracy: 26.579999923706055\n",
      "Epoch 3 for 1\n",
      "-------------------------------\n",
      "Memory used: 47.0%\n",
      "Best model saved for 1.\n",
      "Iteration: 4000. Loss: 2.009331703186035. Accuracy: 26.950000762939453\n",
      "Best model saved for 1.\n",
      "Iteration: 4500. Loss: 2.1563432216644287. Accuracy: 27.09000015258789\n",
      "Best model saved for 1.\n",
      "Iteration: 5000. Loss: 1.8431282043457031. Accuracy: 28.5\n",
      "Best model saved for 1.\n",
      "Iteration: 5500. Loss: 1.8709471225738525. Accuracy: 29.0\n",
      "Epoch 4 for 1\n",
      "-------------------------------\n",
      "Memory used: 51.6%\n",
      "Iteration: 6000. Loss: 2.1580886840820312. Accuracy: 24.989999771118164\n",
      "Iteration: 6500. Loss: 1.752091407775879. Accuracy: 28.610000610351562\n",
      "Best model saved for 1.\n",
      "Iteration: 7000. Loss: 2.0500433444976807. Accuracy: 29.670000076293945\n",
      "Iteration: 7500. Loss: 1.7953755855560303. Accuracy: 29.020000457763672\n",
      "Epoch 5 for 1\n",
      "-------------------------------\n",
      "Memory used: 55.9%\n",
      "Best model saved for 1.\n",
      "Iteration: 8000. Loss: 2.0888822078704834. Accuracy: 30.309999465942383\n",
      "Iteration: 8500. Loss: 1.8440526723861694. Accuracy: 30.139999389648438\n",
      "Iteration: 9000. Loss: 1.984807014465332. Accuracy: 30.260000228881836\n",
      "Epoch 6 for 1\n",
      "-------------------------------\n",
      "Memory used: 59.9%\n",
      "Best model saved for 1.\n",
      "Iteration: 9500. Loss: 1.8666716814041138. Accuracy: 32.689998626708984\n",
      "Iteration: 10000. Loss: 1.9368034601211548. Accuracy: 30.100000381469727\n",
      "Iteration: 10500. Loss: 1.78792142868042. Accuracy: 32.5\n",
      "Iteration: 11000. Loss: 2.1431894302368164. Accuracy: 30.8700008392334\n",
      "Epoch 7 for 1\n",
      "-------------------------------\n",
      "Memory used: 64.4%\n",
      "Iteration: 11500. Loss: 1.4041006565093994. Accuracy: 30.260000228881836\n",
      "Early stopping triggered for 1. Training stopped. \n",
      "\n",
      "Iteration: 12000. Loss: 1.5420202016830444. Accuracy: 31.93000030517578\n",
      "Epoch 8 for 1\n",
      "-------------------------------\n",
      "Memory used: 66.1%\n",
      "Early stopping triggered for 1. Training stopped. \n",
      "\n",
      "Iteration: 12500. Loss: 2.1777377128601074. Accuracy: 31.100000381469727\n",
      "Epoch 9 for 1\n",
      "-------------------------------\n",
      "Memory used: 67.3%\n",
      "Early stopping triggered for 1. Training stopped. \n",
      "\n",
      "Iteration: 13000. Loss: 1.8214352130889893. Accuracy: 30.579999923706055\n",
      "Epoch 10 for 1\n",
      "-------------------------------\n",
      "Memory used: 68.4%\n",
      "Early stopping triggered for 1. Training stopped. \n",
      "\n",
      "Iteration: 13500. Loss: 1.570743441581726. Accuracy: 30.940000534057617\n",
      "Epoch 11 for 1\n",
      "-------------------------------\n",
      "Memory used: 69.6%\n",
      "Early stopping triggered for 1. Training stopped. \n",
      "\n",
      "Iteration: 14000. Loss: 2.0569097995758057. Accuracy: 30.530000686645508\n",
      "Epoch 12 for 1\n",
      "-------------------------------\n",
      "Memory used: 70.7%\n",
      "Early stopping triggered for 1. Training stopped. \n",
      "\n",
      "Iteration: 14500. Loss: 1.5448501110076904. Accuracy: 30.739999771118164\n",
      "Epoch 13 for 1\n",
      "-------------------------------\n",
      "Memory used: 71.9%\n",
      "Best model saved for 1.\n",
      "Iteration: 15000. Loss: 1.8304190635681152. Accuracy: 33.209999084472656\n",
      "Iteration: 15500. Loss: 1.736623764038086. Accuracy: 28.579999923706055\n",
      "Iteration: 16000. Loss: 1.9185330867767334. Accuracy: 32.29999923706055\n",
      "Epoch 14 for 1\n",
      "-------------------------------\n",
      "Memory used: 76.4%\n",
      "Iteration: 16500. Loss: 1.723995327949524. Accuracy: 31.6200008392334\n",
      "Iteration: 17000. Loss: 1.6568204164505005. Accuracy: 33.189998626708984\n",
      "Best model saved for 1.\n",
      "Iteration: 17500. Loss: 2.030881643295288. Accuracy: 33.43000030517578\n",
      "Iteration: 18000. Loss: 1.7115510702133179. Accuracy: 33.13999938964844\n",
      "Epoch 15 for 1\n",
      "-------------------------------\n",
      "Memory used: 80.5%\n",
      "Iteration: 18500. Loss: 2.072512149810791. Accuracy: 33.290000915527344\n",
      "Best model saved for 1.\n",
      "Iteration: 19000. Loss: 1.361261010169983. Accuracy: 34.439998626708984\n",
      "Iteration: 19500. Loss: 2.1380977630615234. Accuracy: 32.439998626708984\n",
      "Iteration: 20000. Loss: 1.6402406692504883. Accuracy: 27.940000534057617\n",
      "Epoch 16 for 1\n",
      "-------------------------------\n",
      "Memory used: 84.8%\n",
      "Iteration: 20500. Loss: 1.9959301948547363. Accuracy: 34.13999938964844\n",
      "Iteration: 21000. Loss: 2.0428645610809326. Accuracy: 32.97999954223633\n",
      "Early stopping triggered for 1. Training stopped. \n",
      "\n",
      "Iteration: 21500. Loss: 1.6647224426269531. Accuracy: 33.25\n",
      "Epoch 17 for 1\n",
      "-------------------------------\n",
      "Memory used: 87.8%\n",
      "Early stopping triggered for 1. Training stopped. \n",
      "\n",
      "Iteration: 22000. Loss: 2.062903881072998. Accuracy: 33.779998779296875\n",
      "Epoch 18 for 1\n",
      "-------------------------------\n",
      "Memory used: 88.9%\n",
      "Early stopping triggered for 1. Training stopped. \n",
      "\n",
      "Iteration: 22500. Loss: 1.799203872680664. Accuracy: 33.119998931884766\n",
      "Epoch 19 for 1\n",
      "-------------------------------\n",
      "Memory used: 90.0%\n",
      "Early stopping triggered for 1. Training stopped. \n",
      "\n",
      "Iteration: 23000. Loss: 1.5597777366638184. Accuracy: 33.34000015258789\n",
      "Epoch 20 for 1\n",
      "-------------------------------\n",
      "Memory used: 91.0%\n",
      "Best model saved for 1.\n",
      "Iteration: 23500. Loss: 1.5849902629852295. Accuracy: 34.959999084472656\n",
      "Iteration: 24000. Loss: 1.7482550144195557. Accuracy: 30.549999237060547\n",
      "Iteration: 24500. Loss: 1.724015712738037. Accuracy: 33.5099983215332\n",
      "Epoch 21 for 1\n",
      "-------------------------------\n",
      "Memory used: 95.2%\n",
      "Iteration: 25000. Loss: 1.958378553390503. Accuracy: 33.959999084472656\n",
      "Iteration: 25500. Loss: 1.6742578744888306. Accuracy: 33.84000015258789\n",
      "Best model saved for 1.\n",
      "Iteration: 26000. Loss: 1.8586143255233765. Accuracy: 35.06999969482422\n",
      "Iteration: 26500. Loss: 1.4989820718765259. Accuracy: 33.189998626708984\n",
      "Epoch 22 for 1\n",
      "-------------------------------\n",
      "Memory used: 98.5%\n",
      "Iteration: 27000. Loss: 1.6023374795913696. Accuracy: 32.63999938964844\n",
      "Iteration: 27500. Loss: 1.3890445232391357. Accuracy: 32.650001525878906\n",
      "Iteration: 28000. Loss: 1.7387440204620361. Accuracy: 31.3799991607666\n",
      "Early stopping triggered for 1. Training stopped. \n",
      "\n",
      "Iteration: 28500. Loss: 1.9105889797210693. Accuracy: 31.1299991607666\n",
      "Epoch 23 for 1\n",
      "-------------------------------\n",
      "Memory used: 98.7%\n",
      "Early stopping triggered for 1. Training stopped. \n",
      "\n",
      "Iteration: 29000. Loss: 1.6521720886230469. Accuracy: 34.36000061035156\n",
      "Epoch 24 for 1\n",
      "-------------------------------\n",
      "Memory used: 98.8%\n",
      "Early stopping triggered for 1. Training stopped. \n",
      "\n",
      "Iteration: 29500. Loss: 1.9132627248764038. Accuracy: 29.90999984741211\n",
      "Epoch 25 for 1\n",
      "-------------------------------\n",
      "Memory used: 98.9%\n",
      "Early stopping triggered for 1. Training stopped. \n",
      "\n",
      "Iteration: 30000. Loss: 1.8665311336517334. Accuracy: 34.810001373291016\n",
      "Epoch 26 for 1\n",
      "-------------------------------\n",
      "Memory used: 99.1%\n",
      "Best model saved for 1.\n",
      "Iteration: 30500. Loss: 1.8538446426391602. Accuracy: 35.150001525878906\n",
      "Iteration: 31000. Loss: 1.730932354927063. Accuracy: 34.720001220703125\n",
      "Iteration: 31500. Loss: 1.491805911064148. Accuracy: 34.90999984741211\n",
      "Epoch 27 for 1\n",
      "-------------------------------\n",
      "Memory used: 94.1%\n",
      "Iteration: 32000. Loss: 1.5616402626037598. Accuracy: 34.72999954223633\n",
      "Iteration: 32500. Loss: 2.4261574745178223. Accuracy: 33.75\n",
      "Early stopping triggered for 1. Training stopped. \n",
      "\n",
      "Iteration: 33000. Loss: 1.9873236417770386. Accuracy: 33.0099983215332\n",
      "Epoch 28 for 1\n",
      "-------------------------------\n",
      "Memory used: 97.6%\n",
      "Early stopping triggered for 1. Training stopped. \n",
      "\n",
      "Iteration: 33500. Loss: 1.738167643547058. Accuracy: 34.18000030517578\n",
      "Epoch 29 for 1\n",
      "-------------------------------\n",
      "Memory used: 98.4%\n",
      "Early stopping triggered for 1. Training stopped. \n",
      "\n",
      "Iteration: 34000. Loss: 1.707714319229126. Accuracy: 35.04999923706055\n",
      "Epoch 30 for 1\n",
      "-------------------------------\n",
      "Memory used: 98.6%\n",
      "Early stopping triggered for 1. Training stopped. \n",
      "\n",
      "Iteration: 34500. Loss: 2.0327582359313965. Accuracy: 35.0099983215332\n",
      "Training model RNNModel_1HiddenLayer_ReLU_input_dim=32, hidden_dim=100, layer_dim=2, output_dim=10 with hidden sizes input_dim=32, hidden_dim=100, layer_dim=2, output_dim=10 \n",
      "\n",
      "Epoch 1 for 2\n",
      "-------------------------------\n",
      "Memory used: 98.8%\n",
      "Best model saved for 2.\n",
      "Iteration: 500. Loss: 2.1168243885040283. Accuracy: 19.059999465942383\n",
      "Best model saved for 2.\n",
      "Iteration: 1000. Loss: 2.079061269760132. Accuracy: 22.1200008392334\n",
      "Iteration: 1500. Loss: 2.0428202152252197. Accuracy: 21.1299991607666\n",
      "Epoch 2 for 2\n",
      "-------------------------------\n",
      "Memory used: 98.2%\n",
      "Iteration: 2000. Loss: 2.239572525024414. Accuracy: 16.81999969482422\n",
      "Best model saved for 2.\n",
      "Iteration: 2500. Loss: 2.086843729019165. Accuracy: 27.440000534057617\n",
      "Best model saved for 2.\n",
      "Iteration: 3000. Loss: 1.4894603490829468. Accuracy: 28.6200008392334\n",
      "Iteration: 3500. Loss: 1.9663480520248413. Accuracy: 27.959999084472656\n",
      "Epoch 3 for 2\n",
      "-------------------------------\n",
      "Memory used: 98.9%\n",
      "Iteration: 4000. Loss: 1.9146904945373535. Accuracy: 24.100000381469727\n",
      "Iteration: 4500. Loss: 1.9551715850830078. Accuracy: 23.450000762939453\n",
      "Best model saved for 2.\n",
      "Iteration: 5000. Loss: 1.939520001411438. Accuracy: 28.8799991607666\n",
      "Best model saved for 2.\n",
      "Iteration: 5500. Loss: 1.781204104423523. Accuracy: 30.479999542236328\n",
      "Epoch 4 for 2\n",
      "-------------------------------\n",
      "Memory used: 92.5%\n",
      "Iteration: 6000. Loss: 1.847130537033081. Accuracy: 27.3799991607666\n",
      "Best model saved for 2.\n",
      "Iteration: 6500. Loss: 1.7458834648132324. Accuracy: 31.450000762939453\n",
      "Iteration: 7000. Loss: 1.9213917255401611. Accuracy: 30.270000457763672\n",
      "Iteration: 7500. Loss: 1.7618294954299927. Accuracy: 29.739999771118164\n",
      "Epoch 5 for 2\n",
      "-------------------------------\n",
      "Memory used: 97.9%\n",
      "Iteration: 8000. Loss: 1.9653353691101074. Accuracy: 25.229999542236328\n",
      "Iteration: 8500. Loss: 1.9724575281143188. Accuracy: 30.610000610351562\n",
      "Early stopping triggered for 2. Training stopped. \n",
      "\n",
      "Iteration: 9000. Loss: 2.3652186393737793. Accuracy: 15.520000457763672\n",
      "Epoch 6 for 2\n",
      "-------------------------------\n",
      "Memory used: 96.6%\n",
      "Early stopping triggered for 2. Training stopped. \n",
      "\n",
      "Iteration: 9500. Loss: 1.84382963180542. Accuracy: 26.09000015258789\n",
      "Epoch 7 for 2\n",
      "-------------------------------\n",
      "Memory used: 98.2%\n",
      "Early stopping triggered for 2. Training stopped. \n",
      "\n",
      "Iteration: 10000. Loss: 1.6568975448608398. Accuracy: 30.600000381469727\n",
      "Epoch 8 for 2\n",
      "-------------------------------\n",
      "Memory used: 99.2%\n",
      "Early stopping triggered for 2. Training stopped. \n",
      "\n",
      "Iteration: 10500. Loss: 1.6766225099563599. Accuracy: 30.84000015258789\n",
      "Epoch 9 for 2\n",
      "-------------------------------\n",
      "Memory used: 99.2%\n",
      "Early stopping triggered for 2. Training stopped. \n",
      "\n",
      "Iteration: 11000. Loss: 1.8020637035369873. Accuracy: 29.829999923706055\n",
      "Epoch 10 for 2\n",
      "-------------------------------\n",
      "Memory used: 96.8%\n",
      "Early stopping triggered for 2. Training stopped. \n",
      "\n",
      "Iteration: 11500. Loss: 1.7584648132324219. Accuracy: 31.260000228881836\n",
      "Epoch 11 for 2\n",
      "-------------------------------\n",
      "Memory used: 98.2%\n",
      "Best model saved for 2.\n",
      "Iteration: 12000. Loss: 1.7017486095428467. Accuracy: 32.43000030517578\n",
      "Iteration: 12500. Loss: 1.9905705451965332. Accuracy: 31.3799991607666\n",
      "Iteration: 13000. Loss: 2.2483556270599365. Accuracy: 29.09000015258789\n",
      "Epoch 12 for 2\n",
      "-------------------------------\n",
      "Memory used: 96.9%\n",
      "Iteration: 13500. Loss: 1.6833665370941162. Accuracy: 31.510000228881836\n",
      "Best model saved for 2.\n",
      "Iteration: 14000. Loss: 1.5803418159484863. Accuracy: 34.11000061035156\n",
      "Iteration: 14500. Loss: 2.3791627883911133. Accuracy: 32.939998626708984\n",
      "Iteration: 15000. Loss: 2.111037015914917. Accuracy: 34.06999969482422\n",
      "Epoch 13 for 2\n",
      "-------------------------------\n",
      "Memory used: 95.3%\n",
      "Iteration: 15500. Loss: 2.2581138610839844. Accuracy: 11.029999732971191\n",
      "Iteration: 16000. Loss: 2.2182717323303223. Accuracy: 21.389999389648438\n",
      "Early stopping triggered for 2. Training stopped. \n",
      "\n",
      "Iteration: 16500. Loss: 2.2898762226104736. Accuracy: 28.790000915527344\n",
      "Epoch 14 for 2\n",
      "-------------------------------\n",
      "Memory used: 98.6%\n",
      "Early stopping triggered for 2. Training stopped. \n",
      "\n",
      "Iteration: 17000. Loss: 1.7589056491851807. Accuracy: 30.110000610351562\n",
      "Epoch 15 for 2\n",
      "-------------------------------\n",
      "Memory used: 98.3%\n",
      "Early stopping triggered for 2. Training stopped. \n",
      "\n",
      "Iteration: 17500. Loss: 2.1089370250701904. Accuracy: 31.239999771118164\n",
      "Epoch 16 for 2\n",
      "-------------------------------\n",
      "Memory used: 99.0%\n",
      "Early stopping triggered for 2. Training stopped. \n",
      "\n",
      "Iteration: 18000. Loss: 2.026726722717285. Accuracy: 31.100000381469727\n",
      "Epoch 17 for 2\n",
      "-------------------------------\n",
      "Memory used: 95.4%\n",
      "Early stopping triggered for 2. Training stopped. \n",
      "\n",
      "Iteration: 18500. Loss: 2.1330809593200684. Accuracy: 30.149999618530273\n",
      "Epoch 18 for 2\n",
      "-------------------------------\n",
      "Memory used: 97.4%\n",
      "Early stopping triggered for 2. Training stopped. \n",
      "\n",
      "Iteration: 19000. Loss: 1.5086520910263062. Accuracy: 33.279998779296875\n",
      "Epoch 19 for 2\n",
      "-------------------------------\n",
      "Memory used: 97.5%\n",
      "Early stopping triggered for 2. Training stopped. \n",
      "\n",
      "Iteration: 19500. Loss: 1.5686554908752441. Accuracy: 33.02000045776367\n",
      "Epoch 20 for 2\n",
      "-------------------------------\n",
      "Memory used: 98.6%\n",
      "Early stopping triggered for 2. Training stopped. \n",
      "\n",
      "Iteration: 20000. Loss: 1.7122540473937988. Accuracy: 32.97999954223633\n",
      "Epoch 21 for 2\n",
      "-------------------------------\n",
      "Memory used: 99.0%\n",
      "Early stopping triggered for 2. Training stopped. \n",
      "\n",
      "Iteration: 20500. Loss: 1.535681962966919. Accuracy: 32.27000045776367\n",
      "Epoch 22 for 2\n",
      "-------------------------------\n",
      "Memory used: 90.7%\n",
      "Early stopping triggered for 2. Training stopped. \n",
      "\n",
      "Iteration: 21000. Loss: 1.78885817527771. Accuracy: 25.520000457763672\n",
      "Epoch 23 for 2\n",
      "-------------------------------\n",
      "Memory used: 92.4%\n",
      "Early stopping triggered for 2. Training stopped. \n",
      "\n",
      "Iteration: 21500. Loss: 2.046475887298584. Accuracy: 33.38999938964844\n",
      "Epoch 24 for 2\n",
      "-------------------------------\n",
      "Memory used: 94.2%\n",
      "Early stopping triggered for 2. Training stopped. \n",
      "\n",
      "Iteration: 22000. Loss: 1.824609398841858. Accuracy: 30.6200008392334\n",
      "Epoch 25 for 2\n",
      "-------------------------------\n",
      "Memory used: 95.6%\n",
      "Early stopping triggered for 2. Training stopped. \n",
      "\n",
      "Iteration: 22500. Loss: 2.0961873531341553. Accuracy: 27.950000762939453\n",
      "Epoch 26 for 2\n",
      "-------------------------------\n",
      "Memory used: 96.9%\n",
      "Early stopping triggered for 2. Training stopped. \n",
      "\n",
      "Iteration: 23000. Loss: 1.9432830810546875. Accuracy: 24.200000762939453\n",
      "Epoch 27 for 2\n",
      "-------------------------------\n",
      "Memory used: 96.6%\n",
      "Early stopping triggered for 2. Training stopped. \n",
      "\n",
      "Iteration: 23500. Loss: 1.9589638710021973. Accuracy: 32.040000915527344\n",
      "Epoch 28 for 2\n",
      "-------------------------------\n",
      "Memory used: 98.4%\n",
      "Early stopping triggered for 2. Training stopped. \n",
      "\n",
      "Iteration: 24000. Loss: 2.041065216064453. Accuracy: 33.619998931884766\n",
      "Epoch 29 for 2\n",
      "-------------------------------\n",
      "Memory used: 97.5%\n",
      "Early stopping triggered for 2. Training stopped. \n",
      "\n",
      "Iteration: 24500. Loss: 1.6354178190231323. Accuracy: 32.16999816894531\n",
      "Epoch 30 for 2\n",
      "-------------------------------\n",
      "Memory used: 98.6%\n",
      "Early stopping triggered for 2. Training stopped. \n",
      "\n",
      "Iteration: 25000. Loss: 1.6046388149261475. Accuracy: 34.09000015258789\n",
      "Training model RNNModel_1HiddenLayer_ReLU_input_dim=32, hidden_dim=50, layer_dim=1, output_dim=10 with hidden sizes input_dim=32, hidden_dim=50, layer_dim=1, output_dim=10 \n",
      "\n",
      "Epoch 1 for 3\n",
      "-------------------------------\n",
      "Memory used: 98.5%\n",
      "Best model saved for 3.\n",
      "Iteration: 500. Loss: 2.1660521030426025. Accuracy: 17.65999984741211\n",
      "Best model saved for 3.\n",
      "Iteration: 1000. Loss: 2.096973419189453. Accuracy: 22.56999969482422\n",
      "Iteration: 1500. Loss: 2.1326143741607666. Accuracy: 21.700000762939453\n",
      "Epoch 2 for 3\n",
      "-------------------------------\n",
      "Memory used: 97.0%\n",
      "Best model saved for 3.\n",
      "Iteration: 2000. Loss: 2.292699098587036. Accuracy: 23.1200008392334\n",
      "Best model saved for 3.\n",
      "Iteration: 2500. Loss: 1.9684697389602661. Accuracy: 24.549999237060547\n",
      "Iteration: 3000. Loss: 1.8058253526687622. Accuracy: 18.860000610351562\n",
      "Best model saved for 3.\n",
      "Iteration: 3500. Loss: 1.93723726272583. Accuracy: 25.989999771118164\n",
      "Epoch 3 for 3\n",
      "-------------------------------\n",
      "Memory used: 97.9%\n",
      "Best model saved for 3.\n",
      "Iteration: 4000. Loss: 1.7631659507751465. Accuracy: 27.25\n",
      "Iteration: 4500. Loss: 1.9714922904968262. Accuracy: 26.299999237060547\n",
      "Best model saved for 3.\n",
      "Iteration: 5000. Loss: 1.818232774734497. Accuracy: 27.450000762939453\n",
      "Best model saved for 3.\n",
      "Iteration: 5500. Loss: 1.9329153299331665. Accuracy: 28.68000030517578\n",
      "Epoch 4 for 3\n",
      "-------------------------------\n",
      "Memory used: 98.2%\n",
      "Iteration: 6000. Loss: 1.8951361179351807. Accuracy: 27.65999984741211\n",
      "Best model saved for 3.\n",
      "Iteration: 6500. Loss: 1.7448726892471313. Accuracy: 29.68000030517578\n",
      "Best model saved for 3.\n",
      "Iteration: 7000. Loss: 2.022597074508667. Accuracy: 30.229999542236328\n",
      "Iteration: 7500. Loss: 1.816497564315796. Accuracy: 28.979999542236328\n",
      "Epoch 5 for 3\n",
      "-------------------------------\n",
      "Memory used: 98.0%\n",
      "Iteration: 8000. Loss: 2.1590163707733154. Accuracy: 27.06999969482422\n",
      "Iteration: 8500. Loss: 2.011523962020874. Accuracy: 28.639999389648438\n",
      "Iteration: 9000. Loss: 1.986324667930603. Accuracy: 22.600000381469727\n",
      "Epoch 6 for 3\n",
      "-------------------------------\n",
      "Memory used: 57.4%\n",
      "Early stopping triggered for 3. Training stopped. \n",
      "\n",
      "Iteration: 9500. Loss: 1.864896297454834. Accuracy: 29.1200008392334\n",
      "Epoch 7 for 3\n",
      "-------------------------------\n",
      "Memory used: 45.6%\n",
      "Early stopping triggered for 3. Training stopped. \n",
      "\n",
      "Iteration: 10000. Loss: 1.4570013284683228. Accuracy: 28.920000076293945\n",
      "Epoch 8 for 3\n",
      "-------------------------------\n",
      "Memory used: 18.4%\n",
      "Best model saved for 3.\n",
      "Iteration: 10500. Loss: 1.5362439155578613. Accuracy: 30.3700008392334\n",
      "Iteration: 11000. Loss: 1.9632493257522583. Accuracy: 29.549999237060547\n",
      "Best model saved for 3.\n",
      "Iteration: 11500. Loss: 2.0948524475097656. Accuracy: 31.3799991607666\n",
      "Epoch 9 for 3\n",
      "-------------------------------\n",
      "Memory used: 21.9%\n",
      "Iteration: 12000. Loss: 1.9327051639556885. Accuracy: 30.81999969482422\n",
      "Iteration: 12500. Loss: 1.712755799293518. Accuracy: 31.190000534057617\n",
      "Iteration: 13000. Loss: 1.6785404682159424. Accuracy: 31.360000610351562\n",
      "Best model saved for 3.\n",
      "Iteration: 13500. Loss: 2.088533401489258. Accuracy: 31.520000457763672\n",
      "Epoch 10 for 3\n",
      "-------------------------------\n",
      "Memory used: 26.6%\n",
      "Iteration: 14000. Loss: 1.8413214683532715. Accuracy: 31.09000015258789\n",
      "Best model saved for 3.\n",
      "Iteration: 14500. Loss: 1.8319257497787476. Accuracy: 32.529998779296875\n",
      "Best model saved for 3.\n",
      "Iteration: 15000. Loss: 1.9236286878585815. Accuracy: 33.22999954223633\n",
      "Iteration: 15500. Loss: 1.7810325622558594. Accuracy: 32.81999969482422\n",
      "Epoch 11 for 3\n",
      "-------------------------------\n",
      "Memory used: 31.7%\n",
      "Iteration: 16000. Loss: 1.5211949348449707. Accuracy: 29.65999984741211\n",
      "Iteration: 16500. Loss: 1.8627415895462036. Accuracy: 32.54999923706055\n",
      "Iteration: 17000. Loss: 1.7383571863174438. Accuracy: 33.099998474121094\n",
      "Early stopping triggered for 3. Training stopped. \n",
      "\n",
      "Iteration: 17500. Loss: 2.194270372390747. Accuracy: 32.540000915527344\n",
      "Epoch 12 for 3\n",
      "-------------------------------\n",
      "Memory used: 36.2%\n",
      "Early stopping triggered for 3. Training stopped. \n",
      "\n",
      "Iteration: 18000. Loss: 1.7930207252502441. Accuracy: 29.139999389648438\n",
      "Epoch 13 for 3\n",
      "-------------------------------\n",
      "Memory used: 37.4%\n",
      "Early stopping triggered for 3. Training stopped. \n",
      "\n",
      "Iteration: 18500. Loss: 1.7553592920303345. Accuracy: 31.459999084472656\n",
      "Epoch 14 for 3\n",
      "-------------------------------\n",
      "Memory used: 38.6%\n",
      "Early stopping triggered for 3. Training stopped. \n",
      "\n",
      "Iteration: 19000. Loss: 1.8417011499404907. Accuracy: 31.790000915527344\n",
      "Epoch 15 for 3\n",
      "-------------------------------\n",
      "Memory used: 40.8%\n",
      "Early stopping triggered for 3. Training stopped. \n",
      "\n",
      "Iteration: 19500. Loss: 2.2789504528045654. Accuracy: 27.979999542236328\n",
      "Epoch 16 for 3\n",
      "-------------------------------\n",
      "Memory used: 42.1%\n",
      "Early stopping triggered for 3. Training stopped. \n",
      "\n",
      "Iteration: 20000. Loss: 1.655698537826538. Accuracy: 30.56999969482422\n",
      "Epoch 17 for 3\n",
      "-------------------------------\n",
      "Memory used: 43.3%\n",
      "Early stopping triggered for 3. Training stopped. \n",
      "\n",
      "Iteration: 20500. Loss: 1.9334192276000977. Accuracy: 32.119998931884766\n",
      "Epoch 18 for 3\n",
      "-------------------------------\n",
      "Memory used: 44.5%\n",
      "Early stopping triggered for 3. Training stopped. \n",
      "\n",
      "Iteration: 21000. Loss: 1.9710942506790161. Accuracy: 33.150001525878906\n",
      "Epoch 19 for 3\n",
      "-------------------------------\n",
      "Memory used: 45.8%\n",
      "Best model saved for 3.\n",
      "Iteration: 21500. Loss: 1.8322155475616455. Accuracy: 33.61000061035156\n",
      "Iteration: 22000. Loss: 2.1321465969085693. Accuracy: 29.06999969482422\n",
      "Iteration: 22500. Loss: 1.9911561012268066. Accuracy: 31.049999237060547\n",
      "Epoch 20 for 3\n",
      "-------------------------------\n",
      "Memory used: 50.2%\n",
      "Best model saved for 3.\n",
      "Iteration: 23000. Loss: 1.4840949773788452. Accuracy: 33.88999938964844\n",
      "Best model saved for 3.\n",
      "Iteration: 23500. Loss: 1.9292972087860107. Accuracy: 33.97999954223633\n",
      "Iteration: 24000. Loss: 1.9050118923187256. Accuracy: 33.189998626708984\n",
      "Iteration: 24500. Loss: 1.554233431816101. Accuracy: 32.130001068115234\n",
      "Epoch 21 for 3\n",
      "-------------------------------\n",
      "Memory used: 54.7%\n",
      "Iteration: 25000. Loss: 1.9306591749191284. Accuracy: 30.6299991607666\n",
      "Iteration: 25500. Loss: 1.868625283241272. Accuracy: 33.220001220703125\n",
      "Early stopping triggered for 3. Training stopped. \n",
      "\n",
      "Iteration: 26000. Loss: 1.7711418867111206. Accuracy: 33.790000915527344\n",
      "Epoch 22 for 3\n",
      "-------------------------------\n",
      "Memory used: 57.8%\n",
      "Early stopping triggered for 3. Training stopped. \n",
      "\n",
      "Iteration: 26500. Loss: 1.4983744621276855. Accuracy: 33.63999938964844\n",
      "Epoch 23 for 3\n",
      "-------------------------------\n",
      "Memory used: 59.3%\n",
      "Early stopping triggered for 3. Training stopped. \n",
      "\n",
      "Iteration: 27000. Loss: 1.7049086093902588. Accuracy: 33.380001068115234\n",
      "Epoch 24 for 3\n",
      "-------------------------------\n",
      "Memory used: 60.4%\n",
      "Early stopping triggered for 3. Training stopped. \n",
      "\n",
      "Iteration: 27500. Loss: 1.662773847579956. Accuracy: 31.040000915527344\n",
      "Epoch 25 for 3\n",
      "-------------------------------\n",
      "Memory used: 61.6%\n",
      "Early stopping triggered for 3. Training stopped. \n",
      "\n",
      "Iteration: 28000. Loss: 1.814984917640686. Accuracy: 33.459999084472656\n",
      "Epoch 26 for 3\n",
      "-------------------------------\n",
      "Memory used: 62.2%\n",
      "Early stopping triggered for 3. Training stopped. \n",
      "\n",
      "Iteration: 28500. Loss: 1.7093911170959473. Accuracy: 31.600000381469727\n",
      "Epoch 27 for 3\n",
      "-------------------------------\n",
      "Memory used: 63.4%\n",
      "Early stopping triggered for 3. Training stopped. \n",
      "\n",
      "Iteration: 29000. Loss: 1.725022315979004. Accuracy: 33.95000076293945\n",
      "Epoch 28 for 3\n",
      "-------------------------------\n",
      "Memory used: 64.5%\n",
      "Best model saved for 3.\n",
      "Iteration: 29500. Loss: 1.801745891571045. Accuracy: 34.08000183105469\n",
      "Iteration: 30000. Loss: 2.0584497451782227. Accuracy: 33.02000045776367\n",
      "Iteration: 30500. Loss: 1.7762401103973389. Accuracy: 33.4900016784668\n",
      "Epoch 29 for 3\n",
      "-------------------------------\n",
      "Memory used: 68.7%\n",
      "Best model saved for 3.\n",
      "Iteration: 31000. Loss: 1.6848068237304688. Accuracy: 34.34000015258789\n",
      "Iteration: 31500. Loss: 2.256308078765869. Accuracy: 33.790000915527344\n",
      "Iteration: 32000. Loss: 1.8951382637023926. Accuracy: 31.549999237060547\n",
      "Iteration: 32500. Loss: 2.0320887565612793. Accuracy: 34.13999938964844\n",
      "Epoch 30 for 3\n",
      "-------------------------------\n",
      "Memory used: 73.1%\n",
      "Best model saved for 3.\n",
      "Iteration: 33000. Loss: 1.967904806137085. Accuracy: 34.349998474121094\n",
      "Iteration: 33500. Loss: 2.0328283309936523. Accuracy: 32.56999969482422\n",
      "Best model saved for 3.\n",
      "Iteration: 34000. Loss: 2.228745698928833. Accuracy: 34.43000030517578\n",
      "Iteration: 34500. Loss: 1.9562346935272217. Accuracy: 31.93000030517578\n",
      "Training model RNNModel_1HiddenLayer_ReLU_input_dim=32, hidden_dim=200, layer_dim=1, output_dim=10 with hidden sizes input_dim=32, hidden_dim=200, layer_dim=1, output_dim=10 \n",
      "\n",
      "Epoch 1 for 4\n",
      "-------------------------------\n",
      "Memory used: 77.6%\n",
      "Best model saved for 4.\n",
      "Iteration: 500. Loss: 2.2391438484191895. Accuracy: 13.479999542236328\n",
      "Best model saved for 4.\n",
      "Iteration: 1000. Loss: 2.1344950199127197. Accuracy: 18.020000457763672\n",
      "Best model saved for 4.\n",
      "Iteration: 1500. Loss: 2.0405962467193604. Accuracy: 20.719999313354492\n",
      "Epoch 2 for 4\n",
      "-------------------------------\n",
      "Memory used: 81.6%\n",
      "Best model saved for 4.\n",
      "Iteration: 2000. Loss: 2.4310526847839355. Accuracy: 22.170000076293945\n",
      "Best model saved for 4.\n",
      "Iteration: 2500. Loss: 1.9109394550323486. Accuracy: 24.559999465942383\n",
      "Iteration: 3000. Loss: 1.8797732591629028. Accuracy: 23.030000686645508\n",
      "Iteration: 3500. Loss: 2.0957510471343994. Accuracy: 24.079999923706055\n",
      "Epoch 3 for 4\n",
      "-------------------------------\n",
      "Memory used: 86.2%\n",
      "Iteration: 4000. Loss: 1.9459519386291504. Accuracy: 23.18000030517578\n",
      "Best model saved for 4.\n",
      "Iteration: 4500. Loss: 2.0496933460235596. Accuracy: 24.979999542236328\n",
      "Best model saved for 4.\n",
      "Iteration: 5000. Loss: 1.8460670709609985. Accuracy: 27.020000457763672\n",
      "Iteration: 5500. Loss: 2.107351779937744. Accuracy: 27.0\n",
      "Epoch 4 for 4\n",
      "-------------------------------\n",
      "Memory used: 90.5%\n",
      "Iteration: 6000. Loss: 2.1536834239959717. Accuracy: 24.639999389648438\n",
      "Best model saved for 4.\n",
      "Iteration: 6500. Loss: 1.7282719612121582. Accuracy: 28.139999389648438\n",
      "Best model saved for 4.\n",
      "Iteration: 7000. Loss: 1.9871376752853394. Accuracy: 28.700000762939453\n",
      "Iteration: 7500. Loss: 1.5581865310668945. Accuracy: 27.770000457763672\n",
      "Epoch 5 for 4\n",
      "-------------------------------\n",
      "Memory used: 94.9%\n",
      "Iteration: 8000. Loss: 2.1736536026000977. Accuracy: 26.270000457763672\n",
      "Iteration: 8500. Loss: 2.163825035095215. Accuracy: 27.579999923706055\n",
      "Best model saved for 4.\n",
      "Iteration: 9000. Loss: 1.900250792503357. Accuracy: 29.200000762939453\n",
      "Epoch 6 for 4\n",
      "-------------------------------\n",
      "Memory used: 91.3%\n",
      "Best model saved for 4.\n",
      "Iteration: 9500. Loss: 2.0118212699890137. Accuracy: 30.1200008392334\n",
      "Best model saved for 4.\n",
      "Iteration: 10000. Loss: 2.0678329467773438. Accuracy: 31.610000610351562\n",
      "Best model saved for 4.\n",
      "Iteration: 10500. Loss: 1.8124228715896606. Accuracy: 32.650001525878906\n",
      "Iteration: 11000. Loss: 1.9023594856262207. Accuracy: 29.549999237060547\n",
      "Epoch 7 for 4\n",
      "-------------------------------\n",
      "Memory used: 95.3%\n",
      "Iteration: 11500. Loss: 1.6483769416809082. Accuracy: 31.610000610351562\n",
      "Iteration: 12000. Loss: 1.528070330619812. Accuracy: 32.15999984741211\n",
      "Iteration: 12500. Loss: 2.047004461288452. Accuracy: 31.229999542236328\n",
      "Early stopping triggered for 4. Training stopped. \n",
      "\n",
      "Iteration: 13000. Loss: 1.927619457244873. Accuracy: 32.380001068115234\n",
      "Epoch 8 for 4\n",
      "-------------------------------\n",
      "Memory used: 94.5%\n",
      "Early stopping triggered for 4. Training stopped. \n",
      "\n",
      "Iteration: 13500. Loss: 2.014341115951538. Accuracy: 29.700000762939453\n",
      "Epoch 9 for 4\n",
      "-------------------------------\n",
      "Memory used: 95.5%\n",
      "Early stopping triggered for 4. Training stopped. \n",
      "\n",
      "Iteration: 14000. Loss: 2.1204898357391357. Accuracy: 30.889999389648438\n",
      "Epoch 10 for 4\n",
      "-------------------------------\n",
      "Memory used: 96.6%\n",
      "Early stopping triggered for 4. Training stopped. \n",
      "\n",
      "Iteration: 14500. Loss: 1.5453861951828003. Accuracy: 30.270000457763672\n",
      "Epoch 11 for 4\n",
      "-------------------------------\n",
      "Memory used: 96.9%\n",
      "Best model saved for 4.\n",
      "Iteration: 15000. Loss: 2.1797780990600586. Accuracy: 32.810001373291016\n",
      "Iteration: 15500. Loss: 1.935167670249939. Accuracy: 28.540000915527344\n",
      "Iteration: 16000. Loss: 1.8422961235046387. Accuracy: 30.1200008392334\n",
      "Epoch 12 for 4\n",
      "-------------------------------\n",
      "Memory used: 95.0%\n",
      "Iteration: 16500. Loss: 2.207300901412964. Accuracy: 32.38999938964844\n",
      "Iteration: 17000. Loss: 1.9705166816711426. Accuracy: 29.6299991607666\n",
      "Early stopping triggered for 4. Training stopped. \n",
      "\n",
      "Iteration: 17500. Loss: 2.06992244720459. Accuracy: 29.81999969482422\n",
      "Epoch 13 for 4\n",
      "-------------------------------\n",
      "Memory used: 97.0%\n",
      "Early stopping triggered for 4. Training stopped. \n",
      "\n",
      "Iteration: 18000. Loss: 1.6811333894729614. Accuracy: 32.09000015258789\n",
      "Epoch 14 for 4\n",
      "-------------------------------\n",
      "Memory used: 91.7%\n",
      "Early stopping triggered for 4. Training stopped. \n",
      "\n",
      "Iteration: 18500. Loss: 2.1812984943389893. Accuracy: 31.420000076293945\n",
      "Epoch 15 for 4\n",
      "-------------------------------\n",
      "Memory used: 92.8%\n",
      "Early stopping triggered for 4. Training stopped. \n",
      "\n",
      "Iteration: 19000. Loss: 2.0378613471984863. Accuracy: 31.649999618530273\n",
      "Epoch 16 for 4\n",
      "-------------------------------\n",
      "Memory used: 93.6%\n",
      "Best model saved for 4.\n",
      "Iteration: 19500. Loss: 1.8360884189605713. Accuracy: 33.439998626708984\n",
      "Iteration: 20000. Loss: 1.5730425119400024. Accuracy: 32.9900016784668\n",
      "Iteration: 20500. Loss: 1.7223536968231201. Accuracy: 32.939998626708984\n",
      "Epoch 17 for 4\n",
      "-------------------------------\n",
      "Memory used: 96.7%\n",
      "Iteration: 21000. Loss: 1.6287150382995605. Accuracy: 33.36000061035156\n",
      "Best model saved for 4.\n",
      "Iteration: 21500. Loss: 1.7298167943954468. Accuracy: 34.470001220703125\n",
      "Iteration: 22000. Loss: 1.6855651140213013. Accuracy: 33.709999084472656\n",
      "Iteration: 22500. Loss: 1.9867041110992432. Accuracy: 25.59000015258789\n",
      "Epoch 18 for 4\n",
      "-------------------------------\n",
      "Memory used: 94.1%\n",
      "Iteration: 23000. Loss: 2.076188564300537. Accuracy: 25.850000381469727\n",
      "Iteration: 23500. Loss: 2.26167893409729. Accuracy: 29.93000030517578\n",
      "Early stopping triggered for 4. Training stopped. \n",
      "\n",
      "Iteration: 24000. Loss: 2.244173526763916. Accuracy: 32.65999984741211\n",
      "Epoch 19 for 4\n",
      "-------------------------------\n",
      "Memory used: 96.3%\n",
      "Early stopping triggered for 4. Training stopped. \n",
      "\n",
      "Iteration: 24500. Loss: 1.6318089962005615. Accuracy: 31.84000015258789\n",
      "Epoch 20 for 4\n",
      "-------------------------------\n",
      "Memory used: 97.1%\n",
      "Early stopping triggered for 4. Training stopped. \n",
      "\n",
      "Iteration: 25000. Loss: 1.5454288721084595. Accuracy: 33.13999938964844\n",
      "Epoch 21 for 4\n",
      "-------------------------------\n",
      "Memory used: 97.7%\n",
      "Early stopping triggered for 4. Training stopped. \n",
      "\n",
      "Iteration: 25500. Loss: 1.8393738269805908. Accuracy: 33.61000061035156\n",
      "Epoch 22 for 4\n",
      "-------------------------------\n",
      "Memory used: 96.5%\n",
      "Early stopping triggered for 4. Training stopped. \n",
      "\n",
      "Iteration: 26000. Loss: 1.6702033281326294. Accuracy: 32.189998626708984\n",
      "Epoch 23 for 4\n",
      "-------------------------------\n",
      "Memory used: 97.2%\n",
      "Early stopping triggered for 4. Training stopped. \n",
      "\n",
      "Iteration: 26500. Loss: 1.4419411420822144. Accuracy: 33.91999816894531\n",
      "Epoch 24 for 4\n",
      "-------------------------------\n",
      "Memory used: 97.6%\n",
      "Early stopping triggered for 4. Training stopped. \n",
      "\n",
      "Iteration: 27000. Loss: 2.0939342975616455. Accuracy: 33.290000915527344\n",
      "Epoch 25 for 4\n",
      "-------------------------------\n",
      "Memory used: 92.3%\n",
      "Early stopping triggered for 4. Training stopped. \n",
      "\n",
      "Iteration: 27500. Loss: 1.7211556434631348. Accuracy: 33.599998474121094\n",
      "Epoch 26 for 4\n",
      "-------------------------------\n",
      "Memory used: 93.1%\n",
      "Early stopping triggered for 4. Training stopped. \n",
      "\n",
      "Iteration: 28000. Loss: 1.9970601797103882. Accuracy: 34.220001220703125\n",
      "Epoch 27 for 4\n",
      "-------------------------------\n",
      "Memory used: 94.2%\n",
      "Early stopping triggered for 4. Training stopped. \n",
      "\n",
      "Iteration: 28500. Loss: 2.2809906005859375. Accuracy: 30.079999923706055\n",
      "Epoch 28 for 4\n",
      "-------------------------------\n",
      "Memory used: 95.0%\n",
      "Early stopping triggered for 4. Training stopped. \n",
      "\n",
      "Iteration: 29000. Loss: 1.707659125328064. Accuracy: 33.189998626708984\n",
      "Epoch 29 for 4\n",
      "-------------------------------\n",
      "Memory used: 95.9%\n",
      "Early stopping triggered for 4. Training stopped. \n",
      "\n",
      "Iteration: 29500. Loss: 1.7851241827011108. Accuracy: 32.91999816894531\n",
      "Epoch 30 for 4\n",
      "-------------------------------\n",
      "Memory used: 97.0%\n",
      "Early stopping triggered for 4. Training stopped. \n",
      "\n",
      "Iteration: 30000. Loss: 1.463186502456665. Accuracy: 32.61000061035156\n",
      "All models trained\n"
     ]
    }
   ],
   "source": [
    "rnn_models = []\n",
    "\n",
    "# Define the models\n",
    "rnn_models.append(RNNModel_1HiddenLayer_ReLU(input_dim=32, hidden_dim=100, layer_dim=1, output_dim=10))\n",
    "rnn_models.append(RNNModel_1HiddenLayer_ReLU(input_dim=32, hidden_dim=100, layer_dim=2, output_dim=10))\n",
    "rnn_models.append(RNNModel_1HiddenLayer_ReLU(input_dim=32, hidden_dim=50, layer_dim=1, output_dim=10))\n",
    "rnn_models.append(RNNModel_1HiddenLayer_ReLU(input_dim=32, hidden_dim=200, layer_dim=1, output_dim=10))\n",
    "\n",
    "\n",
    "if len(rnn_experiments) == 0:\n",
    "    for model in rnn_models:\n",
    "        experiment = Experiment(model)\n",
    "        rnn_experiments.append(experiment)\n",
    "        rnn_exec_loop(len(rnn_experiments), experiment)\n",
    "        experiment.free_memory()\n",
    "        \n",
    "print(\"All models trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "502811c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Experiment' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m     precision \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     15\u001b[0m     epochs_used \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(experiment\u001b[38;5;241m.\u001b[39mresult\u001b[38;5;241m.\u001b[39mtrain_loss_history)\n\u001b[1;32m---> 16\u001b[0m     table\u001b[38;5;241m.\u001b[39mappend([i, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_init_params()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_val_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, recall, precision, epochs_used])\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(tabulate(table, headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirstrow\u001b[39m\u001b[38;5;124m'\u001b[39m, tablefmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfancy_grid\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Experiment' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "#experiments = linear_experiments + conv2d_experiments\n",
    "experiments = rnn_experiments\n",
    "\n",
    "# Print the results\n",
    "print(\"Results\")\n",
    "table = [['Row','Model', 'Best Validation Accuracy', 'Recall', 'Precision', 'Epochs']]\n",
    "for i, experiment in enumerate(experiments):\n",
    "    best_val_accuracy = experiment.result.best_val_accuracy\n",
    "    #recall = experiment.result.recall_history[-1]\n",
    "    recall = 'N/A'\n",
    "    #precision = experiment.result.precision_history[-1]\n",
    "    precision = 'N/A'\n",
    "    epochs_used = len(experiment.result.train_loss_history)\n",
    "    table.append([i, f\"{experiment.model.__class__.__name__}_{experiment.model.get_init_params()}\", f\"{best_val_accuracy:.2f}%\", recall, precision, epochs_used])\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
